# Knapsack Problem (Review)

## Introduction
- The Knapsack problem is a classic optimization problem.
- The objective is to maximize the total value of items packed in a knapsack without exceeding a specified weight limit, W.
- Each item has both a weight and a value.

### Problem Definition
- Given a set of items, each with a weight \( w_i \) and a value \( b_i \), the goal is to maximize:
  
  \[
  \text{maximize } \sum_{i=1}^{n} b_i \text{ subject to } \sum_{i=1}^{n} w_i \leq W
  \]

### Example of Items
| Item # | Weight | Value |
|--------|--------|-------|
| 1      | 2      | 3     |
| 2      | 3      | 4     |
| 3      | 4      | 5     |
| 4      | 5      | 6     |

## Brute-force Approach
- The simplest method is to generate all possible combinations of items and select the one with the maximum value that does not exceed the weight limit.
- The complexity of this approach is \( O(2^n) \).

## Dynamic Programming Approach
- A more efficient solution utilizes dynamic programming (DP) to solve overlapping subproblems.

### Identifying Subproblems
- Define a subproblem \( B[k, w] \) as the maximum value that can be obtained using the first \( k \) items with a weight limit of \( w \).
- The recursive formula to compute \( B[k, w] \):

  \[
  B[k, w] = \begin{cases} 
  B[k-1, w] & \text{if } w_k > w \\
  \max(B[k-1, w], B[k-1, w-w_k] + b_k) & \text{if } w_k \leq w 
  \end{cases}
  \]

### Base Cases
- \( B[0, w] = 0 \) for all \( w \) (no items, no value).
- \( B[k, 0] = 0 \) for all \( k \) (no weight capacity, no value).

### Dynamic Programming Algorithm
1. Initialize a table \( B \) of size \( (n+1) \times (W+1) \).
2. Iterate through items and weights, filling the table using the recursive formula.
3. The final solution will be found in \( B[n, W] \).

### Implementation Example
```python
def knapsack(weights, values, W):
    n = len(weights)
    B = [[0 for _ in range(W + 1)] for _ in range(n + 1)]

    for i in range(1, n + 1):
        for w in range(W + 1):
            if weights[i - 1] <= w:
                B[i][w] = max(B[i - 1][w], B[i - 1][w - weights[i - 1]] + values[i - 1])
            else:
                B[i][w] = B[i - 1][w]
                
    return B[n][W]
```

## Finding the Items in the Knapsack
- To determine which items are included in the optimal solution, backtrack through the table:
```python
def find_items(B, weights, values, W):
    n = len(weights)
    items = []
    i, k = n, W
    
    while i > 0 and k > 0:
        if B[i][k] != B[i - 1][k]:  # Item i is included
            items.append(i - 1)
            k -= weights[i - 1]
        i -= 1
        
    return items
```

## Complexity Analysis
- The time complexity of the dynamic programming approach is \( O(nW) \).
- The space complexity is also \( O(nW) \) for the table.

## Different Versions of the Knapsack Problem
1. **0-1 Knapsack Problem**: Each item is either taken whole or not at all (solved with dynamic programming).
2. **Fractional Knapsack Problem**: Items can be divided, and a greedy algorithm is used to achieve an optimal solution.

---

# Matrix Multiplication

## Introduction
- Matrix multiplication is an important operation in computer science and engineering.
- The order of multiplication can significantly affect the computational cost.

## Associative Property
- Matrix multiplication is associative:
  
  \[
  A(BC) = (AB)C
  \]

## Cost of Matrix Multiplication
- If matrix A has dimensions \( m \times n \) and matrix B has dimensions \( n \times k \), the resulting matrix C will have dimensions \( m \times k \).
- The cost of multiplying two matrices is \( O(m \times n \times k) \).

## Matrix Chain Multiplication Problem
- The problem is to find the optimal way to parenthesize a product of matrices to minimize the number of scalar multiplications.

### Recursive Solution
- Let \( m[i, j] \) represent the minimum number of scalar multiplications needed to compute the product of matrices \( A_i \) through \( A_j \).
- The recursive relationship is defined as:

  \[
  m[i, j] = \min_{k=i}^{j-1} (m[i, k] + m[k+1, j] + p_{i-1} \times p_k \times p_j)
  \]

## Dynamic Programming Approach
1. Create a table \( m \) to store the minimum costs.
2. Fill in the table using the recursive relationship.
3. The optimal cost for multiplying matrices \( A_1 \) to \( A_n \) will be in \( m[1, n] \).

### Implementation Example
```python
def matrix_chain_order(p):
    n = len(p) - 1
    m = [[0] * n for _ in range(n)]
    
    for l in range(2, n + 1):  # l is chain length
        for i in range(n - l + 1):
            j = i + l - 1
            m[i][j] = float('inf')
            for k in range(i, j):
                q = m[i][k] + m[k + 1][j] + p[i] * p[k + 1] * p[j + 1]
                if q < m[i][j]:
                    m[i][j] = q
                    
    return m[0][n - 1]
```

## Complexity Analysis
- The time complexity of the matrix chain multiplication problem using dynamic programming is \( O(n^3) \).
- The space complexity is \( O(n^2) \) for the table.

---

## Conclusion
- Both the Knapsack problem and Matrix Multiplication are fundamental topics in algorithms, demonstrating the power of dynamic programming.
- Understanding these concepts is critical for solving complex optimization problems efficiently.