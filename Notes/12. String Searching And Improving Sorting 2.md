---
type: topic
academic_year: 2
subject_code: CSCI203
subject_name: Algorithms And Data Structures
topic_number: 12
title: String Searching And Improving Sorting 2
cssclass: flatBlue, wideTable
---
< [[__CSCI203 MOC.md|Return to CSCI203 MOC]]
## String Search
### Looking for Text (in all the right places)
+ Consider the problem of String Searching:
	+ Given a text, $t$, is the subtext, $s$, present in it?
+ This problem occurs in many real-life applications:
	+ grep; find in a text editor; Genome matching; Google search.
### String Search Problem
+ Example
	+ $t=AGCATGCTGCAGTCATGCTTAGGCTA$
	+ $s=GCT$
	+ s appear three times in t, starting from locations 6, 17, 23.
+ There are a wide number of techniques to achieve this.
### The NaÃ¯ve Approach: Linear Search
+ The simplest possible approach is linear search:
	+ Try to match $s$ starting at each location in $t$.
```pseudocode
for i from 0 to length(t) - length(s)
	Set j to 0
	while j < length(s) do
		if (s[j] != t[i+j]) break
	end while
	if j == length(s) print(" string found starting at location " i)
end for
```
### Linear Search $\ne$ Linear Time Search
+ The outer loop in our algorithm is repeated $|t|-|s|$ times.
+ Typically the string $t$ is much longer than the string $s$, so this is $\theta(t)$
+ The inner loop is repeated up to $|s|$ times for each time round the outer loop
	+ This is $\theta(s)$
+ The total number of comparisons is $\theta(|s|\times|t|)$.
+ The best we can possibly do is $\theta(|s|+|t|)$ - we have to at least look at each string!
### Linear Time Search
+ To achieve the goal of a linear time algorithm this will have to use hashing.
+ We compare the hash of string $s$ with the hash of each substring of $t$ with the same length:
```pseudocode
set hash_s to hash(s)
for i in 0..length(t) - length(s) do
	Set hash_t to hash(t[i..i+length(s)-1])
	if hash_s == hash_t then
		brute-force compare s and the substring
		if they match print (" found starting at location " i)
	end if
end for
```
+ This algorithm takes linear time, provided:
	+ The hash function only collides rarely
	+ The hash function takes constant time to compute
		+ Independent of the length of string $s$!
	+ Surely, the second <u>requirement is impossible!</u>
	+ To hash a string of length $|s|$ must take $\theta(|s|)$ operations. No! 
### Clever Hashing
+ We note that we need $\theta(|s|)$ time to compute $h(s)$.
+ We also need $\theta(|s|)$ time to compute $h(t[0..|s| - 1])$, the initial substring of t.
+ The trick is to compute the hash of each successive substring of $t$ in constant time.
+ If we look closely at these substrings, we see an interesting feature:
	+ Successive substrings differs only by two characters.
+ The first character of the first substring; <span style=color:red>harr</span>
+ The last character of the next substring; <span style=color:red>array</span>
### Rolling Hash
+ Maybe we can define a hash function, given `h("harr)` can compute `h("arry")` in constant time.
+ Let us define a rolling hash function, `r()`, so that:
	+ `h("arry") = r(h("harr"), "h", "y")`
+ We compute the hash of the next substring by removing the first and appending the last characters;
+ If we can compute a rolling hash in constant time then we can do string matching in linear time
### Karp-Rabin String Search
+ The Karp-Rabin algorithm looks like this:
```pseudocode
set hash_s to h(s)
set hash_t to hash(t[0..length(s) - 1])
for i in 0..length(t)-length(s) do
	if hash_s == hash_t then
		brute-force compare s and the substring
		if they match print(" string found starting at location" i)
	end if
	hash_t=roll(hash_t,t[i], t[i+length(s)])
end for
```
+ The function `roll(h, p, s)` computes the rolling hash of the next substring given the , hash of the existing substring $h$, with the prefix $p$ removed and the suffix $s$, appended.
+ We only need to find a suitable function `roll()`
### How we roll
+ One popular way to compute `roll()` is to use something called the <span style=color:red>Rabin fingerprint</span>
+ We start by treating each symbol in the alphabet as an integer - use the ASCII code for example.
+ We then find a random prime number > the size of the alphabet - let's pick $257$
+ We now compute `h("harry")` as
	$$\begin{align}
	257^{3}\times104+257^{2}\times 97+257^{1}\times114+257^{0}\times 114=1,771,793,837
\end{align}$$
+ Note: "h" = 104, "a" = 97 and "r" = 114
### The Next Hash
+ Given that `h("harr")=1,771,793,837`  how do we get `h("arry")`?
	+ Simply compute $r(h,p,s)=257\times(h-257^{3}\times p)+s=257.(1,771,793,837-257^{3}\times104)+121$
	+ In this case the result is $1,654,094,526$ which is exactly the same as `h("arry")`
		$257^{3}\times97+257^{2}\times114+257^{1}\times114+257^{0}\times121$
	+ Note: if these values become too large, we can reduce them to modulo $m$, where $m$ is a convenient value - say $2^{15}$ or $2^{31}$
### Efficient?
+ We can compute our hash values for $s$ and the initial substring of $t$ using compact evaluation.
		$p^{k-1}\times c_{1}+p^{k-2}\times c_2+\ldots+p\times c_{k-1}+c_{k}$
+ This requires a lot of multiplication!
+ It can be rewritten as $h = C_{k} + p(C_{k-1} + p(C_{k-2} + \ldots + p(C_{3} + p(C_{2} + pC_{1}))\ldots))$
+ Where $k=|s|$ and $c_i$ is the $i^\text{th}$ character of s.
+   This requires $|s|-1$ multiplications and $|s|-1$ additions
### Efficiency!
+ If we precompute $q=p^{k-1}$ we can find the next hash value, $h'$ as:
	+ $h'=p\times(h-1\times c_{i)})+c_{j}$ we we remove character $i$ and add character $j$.
+ This requires only 1 multiplication and 1 addition.
	+ Constant time.
+ Thus we have $\theta(|s|)$ operations to perform the initial hashes and $|t|-|s_\times\theta(1)$ operations to do the rehashing.
+ Overall: our algorithm operates in $\theta(|s|+|t|)$ time.
### Sorting
+ We have seen few sorting algorithms
	+ Insertion sort in worst-case takes $O(n^{2})$
	+ Merge Sort in worst-case takes $O(nlogn)$
	+ Heap Sort in worst-case takes $O(nlogn)$
### Two Models for Sorting
+ Comparison-based sorting model
	+ We will see that all comparison sort must take $\theta(nlogn)$ co
+ Other Sorts without relying on comparison
	+ Bucket sort takes $O(n)$ on average
	+ Radix sort also works in linear time
### Lower Bound for Comparison Sort
+ Comparison Sort
	+ Only use comparisons between elements to gain order.
		+ E.g., given two inputs $a_{i}$ and $a_{j}$, we perform one of the comparisons to get the order between them
			+ $a_{i}\le a_{j}$
			+ $a_{i}=a_{j}$
			+ $a_{i}\gt a_{j}$
	+ Examples include, merge sort, heap sort, etc.
### A lower bound for the worst case
+ Theorem:
	+ All comparison sort must take $\theta(nlogn)$ comparisons in the worst case.
### The Decision Tree Model
+ A decision tree is a full binary tree that only represents the comparisons between elements.
+ The comparisons is determined by specific sorting algorithms.
+ Control, data movement and all other inspect aspects of algorithms are ignored.
+ Sort $\{a_{1},a_{2},a_{3}\}$
![400](12.DecisionTrees_Example.png)
#### Decision Tree
+ Each internal node is a comparison of $a_{i}$ and $a_{j}$ 
+ All leaves nodes are all possible orderings of the items
+ The execution of a sorting algorithm corresponds to tracing a simple path from the root of the decision tree down to a leaf.
+ All comparison-based algorithms have an associated decision tree.
![400](12.DecisionTree_InsertionSort.png)
+ To sort n elements,
	+ There will be $n!$ permutations for n elements.
	+ There will be $n!$ leaves in the decision tree.
	+ Any comparison algorithms must be able to produce each permutation of its input with size n, where each permutation will be a leaf node in the decision tree.
	+ Each of the leaf node is reachable from the root.
### What's the running time on a particular input?
![400](12.DecisionTree_runningTimeInput.png)
### What's the running time in the worst case?
![400](12.DecisionTree_RunningTimeWorstCase.png)
### How long is the longest path?
+ The tree has $n!$ leaves.
+ It is a binary tree.
+ We know that for a complete binary tree, the height will be $h=\log_{2}^{\text{No. of leaves}}$
+ The longest path is at least $\log_{2}^{n!}$
+ Using Stirling's formula $n!\sim(\frac{n}{e})^{n}$
	+ $\log_{2}^{n!}\sim\log_{2}(\frac{n}{2})^{n}=n\\log_{2}^{\frac{n}{e}}=\Omega(nlogn)$
### Proof for the lower bound
+ Any comparison sorting algorithm can be represented as a decision tree with $n!$ leaves.
+ The worst running time is the longest length of path in that tree.
+ All decision tree with $n!$ leaves have depth $\Omega(nlogn)$
### Corollary
+ Heapsort and merge sort are asymptotically optimal comparison sorts.
	+ The upper bound for these two types of sorting is $O(nlogn)$, which matches the worst-case lower bound of comparison sorts.
## Sorting in Linear Time (O(1))
+ Some sorting algorithm can run in linear time with specific requirement on its input.
+ Bucket Sorting
	+ Assume all inputs are drawn from uniform distribution with an average-case running time $O(n)$
		+ Works as follows:
			+ Divide input space into n buckets
			+ Distributes n input s into the bucket
### Bucket Sorting
![400](12.BucketSort_Example.png)

```pseudocode
BUCKET-SORT(A)
	Set n to A.length
	let B[0..n - 1] be a new array
	for i= 0 to n - 1 do
		make B[i] an empty list
	end for
	for i = 1 to n do
		insert A[i] into list B[[nA[i]]
	end for
	for i = 0 to n - 1 do
		sort list B[i] with insertion sort
	concatenate the lists B[0], B[1],..., B[n-1] together in order
Each element is chosen from interval [0,1)
```
+ We can analyse that the total running time of bucket sort is $\theta(n)$
+ We can implement each bucket as a linked list, which maintains the order for the element in the list
+ Some issues:
	+ Need to know the domain of elements to be sorted ahead of time.
### Radix Sorting
+ Works on decimal numbers
	+ Each decimal digital has 10 possible values, 0, 1, ..., 9
	+ First sort the least significant digit
	+ Then sort the 2nd, 3rd until the most significant bits
	+ It requires the sort to be stable
![400](12.RadixSort_Example.png)
+ Assume there are n elements in an array A each with d digits
```pseudocode
function RADIX-SORT(A,d) do
	for i = 1 to d then
		use a stable sort to sort array A on digit i
	end for
end
```
![400](12.RadixSort_application.png)
+ To sort n d-digit numbers where each digit is chosen from k possible values, radix sort will take $\theta(d(n+k))$ time if the stable sort is uses $\theta(n+k)$ time.
+ Proof Analysis:
	+ There are d iterations for sorting
	+ Each iteration needs to sort n digits into k buckets, which is $\theta(n+k)$
+ When d is constant and $k=O(n)$, we make radix sort run in linear time.