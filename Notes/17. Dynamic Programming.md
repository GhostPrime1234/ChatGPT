---
type: topic
academic_year: 2
subject_code: CSCI203
subject_name: Algorithms And Data Structures
topic_number: 17
title: Dynamic Programming 1
cssclasses:
  - flatBlue
  - wideTable
subject:
---

< [[__CSCI203 MOC.md|Return to CSCI203 MOC]]
## Dynamic Programming (DP)
+ Dynamic Programming (DP) is a problem solving technique that is:
	+ General
	+ Efficient
	+ Easy to understand.
+ It is applicable to find a wide range of different problems.
+ It usually finds a solution in polynomial time this is a GOOD THING
+ It is often the <span style=color:red>only efficient technique</span> we know for a problem.
+ One way to look at what it is:
	+ Break the problem down into sub-problems
	+ Re-use the solutions to the sub-problems
+ Solving problems defined by recurrences with overlapping sub-problems
+ "*programming*" here means *planning*.
+ Typically applied to optimisation problems
+ Main idea:
	+ Set up a recurrence relating a solution to a larger instance to solutions of some smaller instances.
	+ Solve smaller instances once
	+ Record solutions in a table
	+ Extract solution to the initial instance from that table
### Develop a Dynamic Programming algorithm
+ Developing steps to an optimisation problem:
	1. Characterise the structure of an optimal solution.
	2. Recursively define the value of an optimal solution.
	3. Compute the value of an optimal solution.
	4. Construct an optimal solution from computed information.
+ We can best see how DP works by looking at some examples
## DP #1: Fibonacci Numbers
+ We are all familiar with the Fibonacci numbers:
	+ 1, 1, 2, 3, 5, 8, 13...
+ Each number is defined as the sum of its two immediate predecessors:
	+ $Fib_{1}=Fib_{2}=1$
	+ $Fib_{n}=Fib_{n-1}+Fib_{n-2}$, otherwise
+ We can compute Fibonacci numbers directly from this definition.
### Recursive Fibonacci
```pseudocode
Procedure fib(n: integer): integer
	f: integer
	if (n <= 2) then
		set f to 1
	else
		set f to fib(n-1) + fib(n-2)
	end if
	return f
end procedure fib
```
+ This procedure is correct but is not efficient.
+ It is, in fact, an exponential time algorithm.
+ From the code we can see the time required to compute the $n^{\text{th}}$ Fibonacci number:
	+ $T(n)=T(n-1)+T(n-2)+O(1)$
	+ $T(n)\gt T(n-2)+T(n-2)$
	+ $T(n)\in\theta(2^{\frac{n}{2}})$
+ Interestingly, the time taken to compute the *`n'th`* Fibonacci number is proportional to the *`n'th`* Fibonacci number. e.g. 1, 1, 2, 3, 5, 8, 13
### Further Analysis
+ Let us look at this another way
+ Evaluated $F_{n}$ requires that we evaluate the following tree:
![400](17.FibonacciNumbers_AnalysisTree.png)
+ So to get $F_{n}$ we evaluate:
	+ $F_{n}$ once
	+ $F_{n-1}$ once
	+ $F_{n-2}$ twice
	+ $F_{n-3}$ three times
	+ $F_{n-4}$ five times etc.
+ The cost is in the repeated evaluations of the same thing.
+ What if we only evaluated each of them once?
+ This is the <span style=color:red>key insight</span> in Dynamic Programming!
### Momoization: The heart of DP
+ The recognition that we only need to perform a given calculation once is central to Dynamic Programming.
+ How do we remember the previous evaluations?
	+ We use a dictionary (hash table)
+ Let us look at the DP version of our fib procedure...
### Recursive Fibonacci with Momoization
```procedure
memo: dictionary = {}
Procedure fibDP(n: integer): integer
	f: integer
	if (n in memo) return memo[n]
	if (n <=2) then
		set f to 1
	else
		set f to fibDP(n-1) + fibDP(n-2)
	end if 
	set memo[n] to f
	return f
End procedure fibDP
```
### Analysis
+ Now: We only recurse the first time we evaluate a given Fibonacci number.
+ In all other cases we just looks p the dictionary.
+ Our evaluation tree becomes:
	+ <span style=color:red>Evaluate</span>
	+ <span style=color:green>Memoize</span>
	+ <span style=color:blue>Ignore</span>
![400](17.RecursiveFibonacciDP_analysisTree.png)
+ With this, Dynamic Programming, approach:
	+ We compute $F_{k}$ once for each value $1\le k\le n$
		+ $n$ calls
		+ $O(1)$ per call
	+ We look up $F_{k}$ once for each value $1\le k\le n-1$
		+ $n-1$ calls
		+ $O(1)$ per call
+ So, fibDp takes $O(n)$ time to compute $F(n)$
### In General
+ We can state the general techinque for dynamic programming as follows:
	+ Solve any sub-problems once and memorise (remember) these solutions for later re-use.
+ In essence: DP is <span style=color:red>recursion + memoization</span>
+ The critical problem in using DP is the identification of the sub-problems.
+ The solution time for dynamic programming is derived as follows:
	+ Multiply the number of distinct sub-problems by the solution time per sub-problem;
+ Note: we only solve a sub-problem once.
### Turning DP on its Head
+ Recursion is top down solution.
+ Another way to think about dynamic programming is to look at it as bottom up solution.
+ We can write a bottom up Fibonacci algorithm as follows:
```pseudocode
Procedure fibUp(n: integer): integer
	fib: dictionary = {}
	set k to 1
	repeat
		if k <= 2 then
			set f to 1
		else
			set f to fib[k-1] + fib[k-2]
		end if
		fib[k] = f
		increment k by 1
	until k==n
	return fib[n]
End procedure fibUp
```

| 1   | 1   | ... | `fib[n-2` | `fib[n-1]` | `fib[n]` |
| --- | --- | --- | --------- | ---------- | -------- |
+ Note that this solution completely eliminates the need in calculating the nth Fibonacci number.
+ All dynamic programming algorithms can be transformed in this way
### Bottom Up in General
+ The bottom up approach to DP still involves solving the same set of sub-problems as in the top down approach.
+ What changes is the order in which we solve them.
+ The bottom up order can be considered as a topological sort of the problem's dependency graph
+ For the Fibonacci numbers so the sort order is $F_{1},F_{2},F_{3},\ldots,F_{n-3},F_{n-2},F_{n-1},F_{n}$
![400](17.BottomUpFibNumbers.png)
### Saving Space with DP
+ Often, the bottom up version of dynamic programming allows us to save space (memory) as well as time.
+ As we presented in the algorithm, it used a dictionary containing n entries.
+ In fact, we only ever need the last two values; we can forget the earlier ones.
+ This allows us to re-write the algorithm without explicit memorisation.
## Dynamic Programming #2: Coin-row Problem
+ There is a row of n coins whose values are some positive integers $c_{n},c_{2},\ldots,c_{n}$, not necessarily distinct. The goal is to pick up the maximum amount of money subject to the constrain that <span style=color:red>no two coins adjacent</span> in the initial row can be picked up.
+ e.g.: 5, 1, 2, 10, 6, 2. What is the best selection?
### Dynamic Programming Solution to Coin-row Problem
+ Let $F(n)$ be the maximum amount that can be picked up from the row of n coins. To derive a recurrece for $F(n)$, we partition all the allowed coin selections into two groups:
	+ those without last coin - the max amount is? $F(n-1)$
	+ those with the last coin - the max amount is? $c_{n}+F(n-2)$
+ Thus we have the following recurrence
	+ $F(n)=max\{c_{n}+F(n-2\,F(n-1)\}\ for\ n\gt1$
	+ $F(0)=0, F(1)=c_{1}$
### Algorithm
```pseudocode
CoinRow(C[1..n])
	//Applies formula bottom up to find the maximum amount of money that can be picked up from a coin row without picking two adjacent coins 
	Input: Array C[1..n] of positive integers indicating the coin values
	Output: The maximum amount of money that can be picked up
	F[0]<- 0; F[1] <- C[1]
	for i <- 2 to n do
		F[i] <- max(C[i] + F[i-2], F[i-1])
	return F[n]
```
+ Solving the coin-row problem by dynamic programming for the coin row 5, 1, 2, 10, 6, 2
![400](17.CoinRow_Problem_Output.png)
### Analysis
+ Complexity
	+ Space - $\theta(n)$
	+ Time - $\theta(n)$
+ The algorithm only outputs the maximum total values.
+ We only need to back-trace the computation to see which of the two possibilities, $c_{n}+F(n-2)$ and $F(n-1 )$ produces the maxima
	+ e.g. in the last step, it was $c_{6}+F(4)$ which means coin $c_{6}=2$ is part of the optimal solution
+ An extra array can be used to record the information
### Algorithm
```pseudocode
procedure CoinRow(C[1..n])
	//Applies formula bottom up to find the maximum amount of money that can be picked up from a coin row without picking two adjacent coins 
	//Input: Array C[1..n] of positive integers indicating the coin values 
	//Output: The maximum amount of money that can be , 6, 2 up
	F[0]<-0; F[1]<-C[1]; S[1..n]=0
	for i <- 2 to n do
		// F[i] <- max(C[i] + F[i-2], F[i-1])
		if (C[i] + F[i-2]) > F[i-1]) then
			S[i] = 1
			F[i] = C[i] + F[i-2]
		else
			F[i] = F[i-1]
		end if
	return F[n]
end procedure CoinRow
```
+ Solving the coin-row problem by dynamic programming for the coin row 5,1, 2, 10, 6, 2
![400](17.CoinProblem_AnalysisDiagram2-1.png)![400](17.CoinProblem_AnalysisDiagram2-2.png)
## Dynamic Program #3: Change-making problem
+ Give change for amount n using the minimum number of denominations $d_{1}<d_{2}<\ldots<d_{m}$
+ We consider a dynamic programming algorithm for the general case, assuming availability of unlimited quantities of coins for each of the m denominations $d_1<d_{2}<\ldots<d_{M}$ where $d_{1}=1$.
+ Let $F(n)$ be the minimum number of coins whose values add up to $n;F(0)=0$
+ The amount $n$ can only be obtained by adding one coin of denomination $d_{j}$ to the amount $n-d_{j}$ for $j=1,2,\ldots,m\ s.t.\ n\ge d_{j}$
+ Therefore, we can consider all such denominations and select the one minimizing. $F(n-d_{j})+1$
+ Since 1 is a constant, we can, of course, find the smallest $F(n-d_{j})$ first and then add 1 to it. Hence we have the following recurrence for $F(n)$:
	$$\begin{align*}
	  F(n)=min_{f:n\ge d_{j}}\{F(n-d_{j}\}+1\text{ for }n>2\\
	F(0)=0
	  \end{align*}$$
  + We can compute $F(n)$ by filling a one-row table left to right in the manner similar to the way it was done above for the coin-row problem, but  computing a table entry here requires finding the minimum of up to $m$ numbers.
  + Change making to amount $n=6$ and with denominations 1, 3 and 4;
![400](17.DP3_Change-MakingProblem_arrays.png)
### Algorithm
```pseudocode
procedure ChangeMaking(D[1..m], n)
	//Applies dynamic programming to find the minimum number of coins of denominations ùëëùëë1 < ùëëùëë2 < . . . < ùëëùëëùëöùëö where ùëëùëë1 = 1 that add up to a given amount n
	//Input: Positive integer ùëõùëõ and array ùê∑ùê∑[1. . ùëöùëö] of increasing positive integers indicating the coin denominations where ùê∑ùê∑[1] = 1 
	//Output: The minimum number of coins that add up to n
	F[0]=0
	for i=1 to n do
		temp=infinity; j=1
		while j <= m and i >= D[j] do
			temp = min(F[i - D[j]], temp)
			j = j + 1
		 end while
		 F[i] = temp 
	end for
	return F[n]
end procedure ChangeMaking
```
### Analysis
+ Complexity
	+ Space - $\theta(n)$
	+ Time - $\theta(nm)$
+ To find the coins of an optimal solution, we need to back trace the computations to see which of the denominations produced the minima. 
	+ For the instance considered, the last application of the formula (for $n=6$), the minimum produced by $d2=3$. The second minimum (for $n=6-3$) was also produced for a coin of that denomination. Thus, the minimum-coin set for $n=6$ is two 3's.
## DP #4: Shortest Paths
+ Let us apply the insights we have gained on dynamic programming to a second problem:
	+ Single source, single destination shortest path.
+ We will proceed as follows:
	+ Create a top down, recursive, naive algorithm
	+ Memorize it
	+ Reconstruct it as a bottom up algorithm
+ This is a useful general approach to algorithm design in dynamic programming.
### Step 1: The Naive, Recursive Algorithm
+ In deriving the naive algorithm we need to introduce another key component of dynamic programming guessing!
+ Don't know the answer? Guess!
+ Don't just try any guess - try them all.
+ So, DP = recursion + memoization + guessing
+ The best guess is the answer we are looking for.
### Some Notation for Shortest Paths
+ Remember from last week:
	+ Given a graph, $G=(V,E,W)$, find the shortest path from a starting vertex, $s\in V$, to all other vertices, $v\in V$
	+ $w(u,v)$ is the weight of the edge $(u,v)$
	+ $D(s,v)$ is the length of the shortest path between s and v.
+ If some vertex, $u$, is on the shortest path from $s$ to $v$ then:
	+ $D(s,v)=D(s,u)+D(u,v)$.
+ Specifically, if vertex $u$ immediately precedes vertex $v$ in the shortest path from $s$ to $v$, then: $D(s,v)=D(s,u)+w(u,v)$
+ Our problem is that we don't know which vertex, $u$, to try so we guess-try them all and pick the best.
### The Naive Algorithm
```pseudocode
procedure short(V{}: vertex, E{}: edge, W(): weight, s: vertex, v: vertex)
	if v==s then
		set d to 0
	else
		set d to infinity
		for each u where (u,v) ‚àà E
			set d to min(d, short(V, E, W, s, u) + w(u, v))
		end for
	end if
	return d
end procedure short
```
+ This is a really bad algorithm:
	+ We compute the shortest path between s and every other vertex repeatedly.
+ It is really easy to improve however
+ Memoize the computation.
### The Memoized Algorithm
```pseudocode
D: dictionary {}
procedure shortDP(V{}: vertex, E{}: edge, W(): weight, s: vertex, v: vertex)
	if v == s then
		set d to 0
	else
		set d to infinity
		for each u where (u,v) ‚àà E
			if (u in D) then
				set d to min(d, D[u] + w(u,v))
			else
				set d to min(d, shortDP(V, E, W), s, u) + w(u,v))
			end if
		end for
	end if
	set D[v] to d
	return d
end procedure shortDP
```
### Some Analysis
+ Consider the following graph:
	+ To find the shortest path $D(s,v)$ we proceed as follows:
![400](17.DP4-Analysis_ShortestPathGraph1.png)![400](17.DP4-Analysis_ShortestPathGraph2.png)
+ We now have a problem to find $D(s,v)$ we need to evaluate $D(s,v)$
### A Problem
+ Our "*improved*" algorithm has a problem.
+ It takes infinite time if $G$ has one or more cycles.
+ If $g$ is acyclic the algorithm is $O(|V|+|E|)$
+ We should have anticipated this remember the bottom up formulation.
+ The order of evaluation of sub-problems is a topological sort of the dependency graph.
+ You can only perform a topological sort on a DAG - no cycles allowed.
### Decycling a Graph
+ Is there some way to remove cycles from a graph?
+ Yes, provided none of them are negative cost cycles.
+ We replicate the graph $|V|$ times and construct a new graph as follows:
	+ Eliminate all edges between vertices in the same copy:
		+ If $(u,v)\in E$ in the original graph connect $u_{i}$ to $u_{i+1}$ in the new graph
	+ This is best seen with an example
![400](17.DP4-DecylingGraph1.png)![400](17.DP4-DecylingGraph2.png)
+ This new graph has $|V|^{2}$ vertices and $|V|\times|E|$ edges but it has no cycles.
+ We can now define $D_{k}(s,v)$ as the shortest path from $s$ to $v$ that traverses exactly $k$ edges.
+ The shortest path is now the smallest of the $D_{k}(s,v)$ values
### Analysis
+ We now observe that: $D_{k}(s,v)=min(D_{k-1}(s,u)+w(u,v))$.
+ So, if we use our memoized DP shortest path solution algorithm on this graph we can solve our original problem, even though our graph has cycles.
+ The bottom up version of this $O(|V|\times|E|)$ algorithm is exactly same as the Bellman-Ford algorithm we saw last week.
+ In fact, this is how the Bellman-Ford algorithm was discovered.
## DP #5: Coin-Collection Problem
+ Several coins are placed in cells of an $n\times m$ board, no more than one coin per cell. A robot, located in the upper left cell of the board, needs to collect as many of the coins as possible and bring them to the bottom right cell.
+ On each step the robot can move either one cell to the right or one cell down from its current location. When the robot visits a cell with a coin, it always picks up that coin.
+ Find the maximum number of coins the robot can collect and a path it needs to follow to do this.
![200](17.Coin-CollectionProblem_Matrix1.png)
+ $F(i,j)$ - the largest number of coins the robot can collect and bring to the cell $(i,j)$.
+ It can reach this cell either from the adjacent cell $(i-1,j)$ to the left of it. The largest numbers of coins that can be brought to these cells are $F(i-1,j)$ and $F(i,j-1)$, respectively.
+ $F(i-1,j)$ and $F(i,j-1)$ are equal to 0 for their nonexistent neighbours
![200](17.Coin-CollectionProblem_Matrix2.png)
$$\begin{align*}
\quad F(i,j)=max\{F(i-1,j), F(i,j-1)\}+c_{ij}\ for\ 1\le i\le n,1\le j\le m\\
\quad F(0,j)=0\ for\ 1\le j\le m\text{ and }F(i,0)=0\text{ for }1\le i\le n
\end{align*}$$
+ $c_{ij}=1$ if there is a coin in cell $(i,j)$ and $c_{ij}=0$ otherwise
![300](17.CoinCollection_Example.png)
+ (a) Coins to collect.
+ (b) Dynamic programming algorithm results.
+ (c) Two paths to collect 5 coins, the maximum number of coins possible
### Algorithm
```pseudocode
RobotCoinCollection(C[1..n, 1..m])
	//Applies dynamic programming to compute the largest number of coins a robot can collect on an n √ó m board by starting at (1, 1) and moving right and down from upper left to down right corner
	//Input: Matrix C[1..n, 1..m] whose elements are equal to 1 and 0
	//for cells with and without a coin, respectively
	//Output: Largest number of coins the robot can bring to cell (n, m)

	set F[1, 1] to C[1, 1]
	for j = 2 to m do
		set F[1, j] to F[1, j-1] + C[i, j]
	for i = 2 to n do
		set F[i, 1] to F[i - 1, 1] + C[i, 1]
		for j <- 2 to m do
			set F[i, j] to max(F[i - 1, j], F[i, j - 1]) + C[i, j]
	return F[n, m]
```
### Shortest Paths
+ Let us apply the insights we have gained on dynamic programming to find
	+ Single source, single destination shortest path.
+ We will proceed as follows:
	+ Create a top down, recursive, naive algorithm.
	+ Memorise it
### Step 1: The Naive, Recursive Algorithm
+ In deriving the naive algorithm we need to introduce another key component of dynamic programming guessing!
+ Don't know the answer? Guess
+ Don't just try any guess - try them all.
+ So, DP = recursion + memorization + guessing
### Some Notation for Shortest paths
+ Remember from last week:
	+ Given a graph, $G=(V,E,W)$, find the shortest path from a starting vertex, $s\in V$, to all other vertices, $v\in V$
	+ $w(u,v)$ is the weight of the edge $(u,v)$
	+ $D(s,v)$ is the length of the shortest path between s and v.
+ If some vertex, $u$, is on the shortest path from $s$ to $v$ then:
	+ $D(s,v)=D(s,u)+D(u,v)$.
+ Specifically, if vertex $u$ immediately precedes vertex $v$ in the shortest path from $s$ to $v$, then: $D(s,v)=D(s,u)+w(u,v)$
+ Our problem is that we don't know which vertex, $u$, to try so we guess-try them all and pick the best.
### The Naive Algorithm
```pseudocode
procedure short(V{}: vertex, E{}: edge, W(): weight, s: vertex, v: vertex)
	if v==s then
		set d to 0
	else
		set d to infinity
		for each u where (u,v) ‚àà E
			set d to min(d, short(V, E, W, s, u) + w(u, v))
		end for
	end if
	return d
end procedure short
```
+ This is a really bad algorithm:
	+ We compute the shortest path between s and every other vertex repeatedly.
+ It is really easy to improve however; Memorise the computation.
### Step 2: The Memoised Algorithm
```pseudocode
D: dictionary {}
procedure shortDP(V{}: vertex, E{}: edge, W(): weight, s: vertex, v: vertex)
	if v == s then
		set d to 0
	else
		set d to infinity
		for each u where (u,v) ‚àà E
			if (u in D) then
				set d to min(d, D[u] + w(u,v))
			else
				set d to min(d, shortDP(V, E, W), s, u) + w(u,v))
			end if
		end for
	end if
	set D[v] to d
	return d
end procedure shortDP
```
### Analysis
+ This algorithm for acyclic group
	+ It takes infinite time if $G$ has one or more cycles.
+ If $g$ is acyclic the algorithm is $O(|V|+|E|)$
## DP #5: Rod-Cutting Problem
+ Given a rod of length $n$ inches and a table of prices $p_{i}$ for $i=1,2,\ldots,n$, determine the maximum revenue $r_{n}$ obtainable by cutting up the rod and selling the pieces.
	+ Note that if the price $p_{n}$ for a rod of length $n$ is large, enough, an optimal solution may require no cutting at all.
+ We will proceed as follows:
	+ Create a top down, recursive algorithm
	+ Memorise it
	+ Reconstruct it as a bottom up algorithm
+ This is a general approach to algorithm design in dynamic programming.
### A rod of lenth $n$ has $2^{n-1}$ cutting options
+ Consider a case $n=4$
 ![300](17.Rod-CuttingProblem_Options.png)
$\text{Optimal revenue } p_{2}+p_{2}=10$
+ A rod of length $n$ in $2^{n-1}$ different ways.
+ For $i=1,2,\ldots,n-1$, we denote a decomposition using ordinary additive notation
	+ $7=2+2+3$ means a rod of 7 length is cut into three pieces, two of length 2 and one of length 3
+ If an optimal solution cuts the rod into $k$ pieces, $1\le ,\le n$, of length $i_{1},i_{2},\ldots,i_{k}$
$$\begin{align}
&n=i_{1}+i_{2}+\ldots+1_k\\
&r_{n}=p_{i_{1}}+p_{i_{2}}+\ldots+p_{i_{k}}
\end{align}$$

| length i    | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  |
| ----------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| price $p_i$ |     |     |     |     |     |     |     |     |     |     |
+ For the sample, we can determine the optimal revenue figures $r_{i}$ for $i=1,2,\ldots,10$ by inspection
	+ ùëü 1=1 from solution 1=1 (no cuts);
	+ 2=5 from solution 2=2 (no cuts);
	+ 3=8 from solution 3=3 (no cuts);
	+ 4=10 from solution 4=2+2;
	+ 5=13 from solution 5=2+3;
	+ 6=17 from solution 6=6 (no cuts);
	+ ùëü 7=18 from solution 7= 1+ 6 ùëúùëúùëúùëú 7= 2+ 2+ 3 ;
	+ ùëü 8= 22 from solution 8= 2+ 6 ;
	+ 9=25 from solution 9= 3+ 6 
	+ 10=10 from solution 10=10 (no cuts) :
+ Generally, $r_{n}$ for $n\ge1$ in terms of optimal revenues from shorter rods
	+ $r_{n}=max(p_{n},r_{1}+r_{n-1}+r_{2}+r_{n-2}+\ldots+r_{n-1}+r_{1})$
	+ $p_{n}$ corresponds to no cut at all
	+ $r_{i}+r_{n-i}$ corresponds to an initial cut into two pieces of size i and $n-i$ for each $i=1,2,\ldots,n-1$, then optimally cut those pieces further to obtain revenues $r_{i}$ and $r_{n-i}$
+ We don't know which $i$ optimises revenue, so we have to consider all possible $i$ and pick the one that optimises the revenue.
+ We also have the option of picking no $i$, i.e. selling the rod without cut.
+ Assumption to make the problem simpler
	+ Piece of length $i$ is left-hand end
	+ Piece of length $n-i$ is the right hand remainder
	+ Only the remainder may be further divided.
+ We view the cut of a length $n$ rod as follows
	+ A first piece followed by some decomposition of the remaineer
	+ $i$ length of cut, $p_{i}$ revenue of the $i$ length of cut
	+ $r_{n-i}$ remainder length
	+ $i=n$ corresponds to no cut, revenue $p_{n}$, remainder has $\text{size }0, r_{0}=0$
			$r_n = \displaystyle\max_{1 \leq i \leq n} \left( p_i + r_{n-i} \right)$
### Recursive Top-down Implementation
```pseudocode
procedure CUT-Rod (p, n)
	if n == 0 then
		return 0
	set q to -infinity
	for i = 1 to n
		set q to max(q; p[i] + CUT-Rob(p; n-i))
	return q
end procedure CUT-Rod
```
### Analysis
+ An example with $n=4$, recursive call tree
![300](17.RodCutting_RecursiveCallTree.png)
+ In general, this recursion tree has $2^{n}$ nodes and $2^{n-1}$ leaves.
+ CUT-Rod explicitly considers all the $2^{n-1}$ possible ways of cutting up a rot of length $n$.
### DP for Optimal Rod-Cutting
+ Top-down with memoisation
```pseudocode
MEMOIZED-CUT-Rob(p, n)
	Input: r[0..n]: array
	for i = 0 to n
		set r[i] to -infinity
	return MEMOISZED-CUT-ROD-AUX(p, n, r)

	procedure MEMOIZED-CUT-ROD-AUX(p, n, r)
		if r[n] > 0 then
			return r[n]
		if n == 0 then
			set q to 0
		else
			set q to -infinity
			for i = 1 to n
				set q to max(q,; p[i] + MEMOISED-CUT-ROD-AUX(p,n-i,r))
			set r[n] to q
		return q
```
### Analysis
+ Complexity
	+ Top-down: $\theta(n^{2})$
	+ Bottom-up: $\theta(n^{2})$
+ Our dynamic programming solutions to the rod-cutting problems return the value of an optimal solution, but they do not return an actual solution
	+ A list of piece sizes
### Reconstruction of the solution
```pseudocode
procedure EXTENDED-BOTTOM-UP-CUT-ROD (p, n)
	INPUT: r[0..n] and s[0..n]: arrays
	set r[0] to 0
	for j = 1 to n
		set q to -infinity
		for i = 1 to j do
			if q < p[i]+r[j-i] then
				set q to p[i]+r[j-i]
				set s[j] to i
			set r[j] to q
		return r and s
```
```pseudocode
PRINT-CUT-ROD-SOLUTION(p, n)
(r, s)=EXTENDED-BOTTOM-UP-CUT-ROD(p, n)
while n > 0 do
	print s[n]
	set n to n - s[n]
```
![300](17.OptimalRodCutting-SolutionReconstruction.png)
+ n = 10, no cut
+ n = 7, cuts 1, 6

## Dynamic Programming
+ Dynamic Programming (DP) is a method for solving complex problems by breaking them down into simpler subproblems.
+ It is applicable when the problem can be divided into overlapping subproblems that can be solved independently.
+ The key idea is to store the results of subproblems to avoid redundant calculations, making DP more efficient than brute-force methods.

## Key Characteristics of Dynamic Programming
1. **Optimal Substructure**: Solutions to subproblems can be used to construct the solution to the overall problem.
2. **Overlapping Subproblems**: The problem can be broken down into smaller subproblems that overlap and can be reused.

## Types of Dynamic Programming
1. **Clever Brute Force**: Optimizing brute force by reducing redundancy.
2. **Top-Down Approach (Memoization)**: Using recursion with stored solutions to avoid recalculating.
3. **Bottom-Up Approach (Tabulation)**: Solving subproblems first and using their solutions to build the final solution iteratively.

### Main Idea
1. Define a recurrence relation for a large instance in terms of smaller instances.
2. Solve smaller instances once and store their results in a table.
3. Use the table to solve the larger instance efficiently.
# #1: Knapsack Problem (Review)
The Knapsack problem asks how to maximize the total value of items packed in a knapsack without exceeding its weight limit.

| Item # | Weight | Value |
|--------|--------|-------|
| 1      | 1      | 8     |
| 2      | 3      | 6     |
| 3      | 5      | 5     |

### Types of Knapsack Problems
1. **0-1 Knapsack Problem**: Items are indivisible; you either take an item or not.
2. **Fractional Knapsack Problem**: Items are divisible, and fractions of items can be taken.

The **0-1 Knapsack Problem** can be solved using dynamic programming, while the **Fractional Knapsack Problem** uses a greedy algorithm.

# O-1 Knapsack Problem
Given a knapsack with a maximum capacity $W$ and a set $S$ of $n$ items, each item $i$ has a weight $w_i$ and a benefit value $b_i$. 
### Objective
Maximize the total value of packed items while keeping the total weight less than or equal to $W$:
$\max \sum_{i \in T} b_i \quad \text{subject to} \quad \sum_{i \in T} w_i \leq W$
The problem is called the **0-1 Knapsack** problem because each item is either accepted or rejected.
# Brute-force Approach
A straightforward brute-force algorithm evaluates all possible combinations of items to find the one with maximum value that meets the weight constraint. 
+ **Complexity**: $O(2^n)$
+ **Can we do better?**: Yes, dynamic programming offers a more efficient solution.
# Defining a Subproblem
Label the items as $1, 2, \ldots, n$. A reasonable subproblem is to find the optimal solution for the subset $S_k = \{\text{items } 1, 2, \ldots, k\}$.

However, the solution for $S_4$ may not be part of the solution for $S_5$ leading to a flawed subproblem definition.

### Revised Subproblem Definition
Let $B[k, w]$ be the value of the most valuable subset of the first \( k \) items that fits into a knapsack of capacity \( w \).
# Recursive Formula
The recursive formula for the subproblems is:
$$B[k,w] =
\begin{cases}
B[k-1, w] & \text{if } (w - w_k) < 0 \\
\max\{B[k-1, w], B[k-1, w - w_k] + b_k\} & \text{if } (w - w_k) \ge 0
\end{cases}$$

### Explanation
- **First case**: If$w - w_k < 0,\ the\ k-th$ item can't be part of the solution.
- **Second case**: If $w - w_k \ge 0$, the optimal solution either includes the $k-th$ item or not, and we choose the maximum value.
# O-1 Knapsack Algorithm
```pseudocode
procedure Knapsack(W, n, b[1..n])
    for w = 0 to W
        B[0, w] = 0
    end for
    for i = 1 to n
        B[i, 0] = 0
        for w = 1 to W
            if w >= w_i
                B[i, w] = max(B[i - 1, w], B[i - 1, w - w_i] + b_i)
            else
                B[i, w] = B[i - 1, w]
        end for
    end for
end procedure
```
### Time Complexity
- The time complexity of this algorithm is \( O(n \times W) \), where \( n \) is the number of items and \( W \) is the capacity of the knapsack.
- This is a significant improvement over the brute-force solution with \( O(2^n) \) complexity.

# Finding Knapsack Items
To find which items are included in the optimal solution:

```pseudocode
i = n, k = W
while i > 0 and k > 0 do
    if B[i, k] != B[i - 1, k]
        mark item i as included
        k = k - w_i
    i = i - 1
end while
```

This algorithm traces back through the table $B$ to determine which items are part of the optimal solution.
## #2 Matrix Multiplication
![400](18.MatrixMultiplication.png)
--- start-multi-column

+ If $A$ has $m$ rows and $n$ columns and B has $n$ rows and $k$ columns then C will have $m$ rows and $k$ columns.
+ Matrix multiplication is not commutative: $AB\ne BA$ ($BA$ may not even exist).
+ The cost of a matrix multiplication depends on the sizes of A and B
+ $A_{m\times n}\times B_{n\times k}$ will take $m\times n\times k$ multiplications and $m\times(n-1)\times k$ additions.
+ The order in which multiple matrices are multiplied will effect the total cost.

--- end-column ---

```pseudocode
procedure MATRIX-MULTIPLY(A,B)
	IF A.columns != B.rows then
		throw error "incompatible dimensions"
	ELSE let C be a new A.rows * B.columns matrix
		FOR i = 1 to A.rows do
			for j = 1 to B.columns
				set c_ij = 0
				for k = 1 to A.columns
					c_ij = i_ij + a_ik * b_kj
	return C
end MATRIX-MULTIPLY
```

--- end-multi-column
### Example
![400](18.MatrixMultiplication_Example.png)
### Matrix-chain Multiplication
+ Matrix multiplication is associative $A(BC)=(AB)C$.
+ Let A, B and C be three matrices with sizes $3\times1,1\times3\text{ and }3\times1$ respectively:
	+ ABC can be computed as $A(BC)\text{ or }(AB)C$
![400](18.Matrix-ChainMultiplication_Diagram.png)
+ Multiplication of four matricies
	+ $<A_{1},A_{2},A_{3}, A_{4}>$
+ We can define five distinct ways to perform the calculation using parentheses:
$(A_{1}(A_{2}(A_{3}A_{4}))),(A_{1}((A_{2}A_{3})A)4 )),((A_{1}A_2)(A_{3}A_{4})),((A_{1}(A_{2}A_{3}))A_{4}),(((A_{1},A_{2})A_{3})A_{4})$
+ How we parenthesize a chain of matrices can have a dramatic impact on the cost of computing the product.
## Generalisation
+ If we need to evaluate the product of $n$ matrices:
	+ $A_{1}\times A_{2}\times A_{3}\times\cdots\times A_{n}$ or $A_{1}A_{2}A_{3}\cdots A_{n}$
+ We can perform this in $O(4^{n})$ different ways, each has a potentially different cost.
+ What is the minimum cost of the overall computation.
	+ What is the optimum sequence of matrix multiplications to perform?
## DP: Parenthesization
+ We can restate the problem as follows:
	+ Given a sequence of $n$ matrices; find the optimal locations for $n-1$ pairs of balanced parentheses, such that each pair contains exactly two matrices or parenthesized sets of matrices
+ E.g., given matrices $ABCD$, possible parenthesizations are: 
	+ $ùê¥ùê¥(ùêµùêµ(ùê∂ùê∂ùê∂ )),ùê¥ùê¥((ùêµùêµùêµ )ùê∑ ),(ùê¥ùê¥ùê¥ )(ùê∂ùê∂ùê∂ ),(ùê¥ùê¥(ùêµùêµùêµ ))ùê∑ ,((ùê¥ùê¥ùê¥ )ùê∂ùê∂)ùê∑$
### Notations
+ Given a chain $<A_{1,}A_{2},\cdots,A_{n}>$ of matrices, where for $i=1,2,\cdots,n$ matrix $A_{i}$ has dimension $p_{i-1}\times p_{i}$, fully parenthesize the product $A_{1}A_{2}\cdots A_{n}$ in a way that minimises the number of scalar multiplications.
	+ $A_{i\cdots j}$ denotes $A_{i}A_{i+1}\cdots A_{j},i\le j$
+ Note: we are not actually multiplying matrices. Our goal is only to determine an order for multiplying matrices that has the lowest cost.
## Step 1. Structure an optimal parenthesization
+ Suppose to optimally parenthesize $A_{i}\cdots A_{j}$
+ Let's say an optimal split is between $A_{k}$ and $A_{k+1}$
+ The problem becomes two sub-problems
	+ $A_{i}A_{i+1}\cdots A_{k}$ and $A_{k+1}A_{k+1}\cdot A_{j}$
+ We must ensure that we search for the correct place to split the product.
	+ We have considered all possible places so that we are sure of having examined the optimal one.
## Step 2. A recursive solution
+ Let $m[i,j]$ be the minimum number of scalar multiplications needed for $A_{i\cdots j}$
	+ For the full problem $A_{1\cdots 1}$, it would be $m[1,n]$
+ If $i=j,A_{i\cdots i}=A_{i}$, no scalar multiplications, $m[i,i]=0$, for $i=1,2,\cdots,n$.
+ If $i<j$ and optimal split at $k$, i.e., $A_{i\cdots k}$ and $A_{k+1\cdots j}$
	+ Scalar multiplications for $A_{i\cdots k}$ is $m[i,k]$
	+ Scalar multiplication for $A_{k+1\cdots j}$ is $m[k+1,j]$
	+ The dimension of $A_{i\cdots k}$ is $p_{i-1}\times p_{k}$
	+ The dimension of $A_{k + 1\cdots j}$ is $p_{k}\times p_{j}$
	+ Scalar multiplications for $A_{i\cdots k}A_{k+1\cdots j}$ is $p_{i-1}p_{k}p_{j}$