---
type: topic
academic_year: 2
subject_code: CSCI203
subject_name: Algorithms And Data Structures
topic_number: 11
title: Hashing
cssclass: flatBlue, wideTable
---
< [[__CSCI203 MOC.md|Return to CSCI203 MOC]]
## Direct Access Table vs Hash Table
---start-multi-column

**Direct Access Table**
![400](11.DirectAccessTable.png)

--- end-column ---

**Hash Table**
![400](11.HashTable.png)

--- end-multi-column
### Hash method works something like
Convert a string key into an integer that will be in the range of 0 through the maximum capacity -1. Assume the array capacity is 9997.
![400](11.HashMethod.png)
### Hashing
![400](11.Hashing.png)
+ Each item has a unique key.
+ Use a large array called a Hash Table.
+ Use a Hash Function
### Example: use ASCII code
![400](11.Hashing_Example-UseASCII_Code.png)
## Operations
+ Initialise
	+ All locations in Hash Table are empty
+ Insert
+ Search
+ Delete
## Hash function
+ Maps keys to positions in the Hash Table.
+ Be easy to calculate.
+ Use all of the keys
+ Spread the keys uniformly.
## Hashing
+ Ideally, if we have $n$ keys with associated values, we would like $m\in \theta(n)$
	+ $m=2n,m=3n$
+ This presents a problem:
	+ Although $m\gt n$, the number of keys we are storing, it is far smaller than that of the number of possible keys.
	+ There will always be circumstances where $\text{key}_{1}\ne\text{key}_2$ but $h(\text{key}_{1})=h(\text{key}_{2})$
	+ This leads to a collision:
		+ Two different keys with the same hash value
		+ Two different keys with the same location in the table.
## Hashing with Chaining
![400](11.Hashing_Chaining.png)
<span style=color:red>m number of slots in T</span>
+ We call $h$ a hash function
+ $h:U\to\{0,1,\ldots,m-1\}$ so that $h(k)$ is a legal slot number in $T$.
+ We say that $k$ hashes to slot $h(k)$
+ Collision
	+ When two or more keys hash to the same slot.
### Chaining
+ Uses a Linked List at each position in the Hash Table
![400](11.Hashing_Chaining-Example.png)
### Insert with Chaining
+ Apply hash functions to get a position in the array.
+ Insert key into the Linked List at this position in the array.
```pseudocode
function InsertChaining(item) do
	posHash = hash(key of item)
	insert(hashTable[posHash], item)
```
### Search with Chaining
+ Apply hash function to get a position in the array.
+ Search the Linked List at this position in the array.
```pseudocode
/* function returns NULL if not found, or the address of the node if found */
function SearchChaining(item) do
	posHash = hash(key of item)
	Node* found
	found = searchList(hashTable[posHash], item)
	return found
```
### Delete with Chaining
+ Apply hash function to get a position in the array.
+ Delete the node in the Linked List at this position in the array.
```pseudocode
/* Function uses the Linked List delete function to delte an item inside that list, it does nothing if that item isn't there. */
function DeleteChaining(item) do
	posHash = hash(key of item)
	deleteList(hashTable[posHash], item)
```
### Advantages of Chaining
+ Insertions and Deletions are easy and quick.
+ Allows more records to be stored.
+ Naturally resizable, allows a varying number of records to be stored.
### Disadvantages of Chaining
+ Uses more space
+ More complex to implement.
	+ A linked list at every element in the array.
## Collision Frequency
+ Birthdays $or$ the von Mises paradox
	+ There are 365 days in a normal year - Birthdays on the same day unlikely?
	+ How many people do I need before "it's an even bet (i.e. the probability > 50%)" that two have the same birthday?
	+ View
		+ The days of the year as the slots in a hash table.
		+ The "birthday function" as mapping people to slots.
	+ Answering von Mises' question answers the question about the probability of collisions in a hash table.
### Distinct Birthdays
+ Let $Q(n)$ = probability that $n$ people have distinct birthdays.
+ $Q(1)=1$
+ With two people, the $2^{nd}$ has only 364 "free birthdays"
	  $Q(2)=Q(1)\times\frac{364}{365}$
  + The 3rd has only 363, and so on:
	  $Q(n)=Q(1)\times\frac{364}{365}\times\frac{363}{365}\times\ldots\frac{365-n+1}{365}$
  ### Coincident Birthdatys
+ Probability of having two identical birthdays
+ $P(n)=1-Q(n)$
+ $P(23)=0.507$
+ With 23 entries, table is only $\frac{23}{365}=6.3\%$ full!
## Hash Tables - Load factor
+ Collisions are very probably!
+ Table load factor must be kept low
--- start-multi-column

$\alpha=\frac{n}{m}$

--- end-column ---

n = number of items
m = number of slots

--- end-multi-column
+ Detailed analyses of the average chain length (or number of comparisons/search) are available.
+ Separate chaining
	+ Linked Lists attached to each slot.
+ Gives best performance
	+ But uses more space!

## Hash Tables - General Design
+ Choose the table size
	+ Large tables reduce the probability of collisions!
	+ Table size, $m$
	+ $n$ items
	+ Collision probability $\alpha=\frac{n}{m}$
+ Choose a table organisation
	+ Does the collection keep growing?
		+ Linked lists (but consider a tree!)
	+ Size relatively static?
		+ Overflow area
		+ Re-hash
+ Choose a hash function
	+ A simple (and fast) one may well be fine
	+ Read your text for some ideas!
+ Check the hash function against your data
	+ Fixed data
		+ Try various $h,m$ until the maximum collision chain is acceptable.
		+ Known performance
	+ Changing data
		+ Choose some representative data.
		+ Try various $h,m$ until collision chain is OK
		+ Usually predictable performance
## Hashing Functions
+ The following are simple approaches which often work reasonably well:
	+ The Division method:
		+ $h(k)=k\text{ mod } m$
		+ Good if $m$ is prime and is not close to a power of $2$ or a power of $10$.
	+ The Multiplication method:
		+ $h(k)=[m(kA\text{ mod }1)]$
		+ $A$ is a constant in the range $0\lt A\lt 1$
		+ $kA\text{ mod }1$ means the fractional part of $kA$, that is $kA-[kA]$.
		+ $m=2^p$ for some integer $p$
+ Universal Hashing:
	+ $h(k)=(a\times k+b\text{ mod }p)\text{ mod }m$
	+ $p$ is a primary number, bigger than $|U|$, the number of all possible keys.
		+ Yes, p is BIG!
	+ $a$ and $b$ are random integers between$ 0$ and $p-1$.
+ This is an excellent hash function.
+ The worst-case probability of two keys colliding is $\frac{1}{m}$.
+ This means that, even if $a$ and $b$ are poorly chosen, this hash function will always work well.
+ The problems of finding a large prime and performing arithmetic on big integers will be left for now.
### Worst Case
+ What if $h(\text{key})$ has the same value for all the keys in our set?
	+ Our hash table has just become a complicated way of storing a single linked list!
	+ Access to a given `key:value` pair is now $O(n)$.
	+ So, should we give up on hashing?
		+ No! In practice this does not happen.
### Best Possible Layout
+ All slots have the same number of keys.
+ Each chain has the same length $\frac{n}{m}$ (load factor)
+ Operations on a dictionary are $O(1 + \frac{n}{m})$
![200](11.Hashing_BestPossibleLayout.png)\
### Picking up $m$
+ $m$ is the number of slots in the dictionary, to be $\theta(n)$, where n is the number of entries in the dictionary.
+ Operations on a dictionary are $O(1 + \frac{n}{m})$, so if n grows too large we get less and less efficient.
+ The problem we face is that, often, we do not know how many records $n$ we will need to store.
	+ If $m$ is too small, the dictionary becomes inefficient.
	+ If $m$ is too large, we waste storage (memory or disk).
+ How do we get the right value for m? Let's say we want $m\ge n$ at all times.
### Lucky Guess?
+ If we have no knowledge of the ultimate size of n, what can we do?
	+ Guess.
	+ Pick $m$ based on an optimistic assessment of the likely size of $n$.
	+ No idea?
		+ Pick you favourite small number. $m=8$, for example.
	+ Now what?
		+ What if $n$ turns out to be greater than 8?
	+ Make m bigger.
		+ How much bigger?
### Changing $m$
+ When $n$ becomes large, change $m$
+ If we change $m$ we have problems:
	+ Our hash array is too small.
	+ Our hashed keys will be wrong. - They depend on the value of $m$.
+ Does this mean that we have to recreate the hash table from scratch?
	+ It sure does. Isn't this a BAD THING?
### Growing (Resizing) a Hash Table
+ What exactly has to happen if we change $m$?
	+ Let's say the new table size is $m'$.
+ We now need a new array with $m'$ elements.
	+ We also need to move all the existing elements from the old table to the new one.
+ Build a new hash function $h'$.
	+ Remember, the hash function depends on $m'$.
+ Insert the existing data into the new table.
	+ This involves re-hashing every key.
+ So, the first question is: How much do we grow $m$?
#### $m'$ =?
+ Let's look at some options:
	+ $m'=m+1$
	+ What is the cost of $n$ insertions?
		+ $\theta(1)$ for the first m insertions.
		+ $\theta(m')$ for each insertion after than.
	+ Overall $\theta(n^{2})$
+ $m'=2m$
	+ $\theta(1)$ for the first m insertions.
	+ $\theta(m)$ for the first insertion
	+ $\theta(1)$ for the  next $m-1$ insertions
	+ $\theta(1)$ for the next insertion
	+ $\theta(2m)$ for the next insertion
	+ $\theta(1)$ for the next $2m-1$ insertions.
+ Overall $\theta(n+(\frac{n}{2})+(\frac{n}{4})+\ldots)=\theta(2n)=\theta(n)$
+ The cost of expanding the table gets spread over the extra elements we are making room for.
+ This is known as Amortised cost.
+ Note: An amortised cost of $\theta(1)$ per operation does not mean that every operation has this cost.
	+ Just that this is the average cost per operation.
### Amortised Cost
+ We say an operation has a cost of "$T(k)$ Amortised" if $k$ operations take a total of $k\times T(k)$ time.
+ Table doubling take $\theta(n)$ operations for $n$ insertions so the amortised cost is $\theta(1)$.
+ Note: We can use table doubling to implement any solution where we don't know the size of the data structure in advance and it grows in a "well behaved" way.
+ Table doubling minimises the cost associated with dynamic data structures.
### Deletions
+ What about deletions?
	+ Each deletion is still $\theta(1)$.
	+ They simply increase the number of operations (insertions and deletions) we can perform between doublings.
+ What if it's all deletions?
	+ In this case the table becomes progressively less and less full.
	+ Solution: Shrink the table.
### Shrinking a Hash Table
+ What should our strategy for reducing the size of the table be?
	+ How about "if $n\lt \frac{m}{2}$ make $m'=\frac{m}{2}$"?
+ What if the next operation is an insertion?
	+ Double the table size!
+ Then a deletion?
	+ Halve the table!
+ Insertion?
	+ Double
+ We have $\theta(n)$ operations for each change in the data.
+ Instead use "if $n <\frac{m}{4}$ make $m'=\frac{m}{2}$".
### Hashing With Chaining Considered Bad
+ There is still one small issue with this method.
	+ We have a hybrid data structure - an array of linked lists.
+ A second approach uses just a simple array.
+ Clearly, we still have a potential problem with collision.
	+ Two keys which hash to the same value.
+ We resolve this with a technique known as Open Addressing.
## Open Addressing
+ An alternative to chaining
+ We wish to hash $n$ items into an array with $m$ slots.
+ We may only store one item per slot.
+ Clearly, $m\ge n$.
+ We insert an item into the table using an iterative technique known as probing.
### Probing
+ This process works as follows: (for insertion)
```pseudocode
Set hash function to starting value, h0
repeat
	calculate prob = hash(key)
	if table(probe) contains data then
		go to the next hash function
	 else
		 store the item in table(probe)
	 end if
 until we have stored the item
```
+ This means we must have a sequence of hash functions, h0, h1, h2 or a hash function which produces a sequence of values.
## The Hash Function
+ Our new hash function requires two arguments:
	+ The key
	+ The iteration count
+ Thus: `probe = OpenHash(key, count)`
+ Here:
	+ key is a valid element of U, the universe of keys.
	+ count is a non-negative integer
	+ As usual, $0\le\text{probe}\lt m-1$
+ In addition, we want our hash function to have the following property:
	+ For any arbitrary key $k$ the sequence of $m$ probes:
		+ $h(k,0),h(k,1),h(k,2),\ldots,h(k,m-1)$
	+ Must be a permutation of the integers:
		+ $0,1,2,\ldots,m-1$
	+ This property guarantees that we must eventually find a vacant slot to insert the item into.
	+ Clearly the sequence of probes must be different for different keys.
### Search with Open Addressing
+ The procedure used to search using open addressing is similar to insertion
```pseudocode
Set count to 0
repeat
	set prob to hash(key, count)
	if table(prob) == key then
		return item
	else
		increment count
	end if
until table(probe) is empty or count == n
return not found
```
### Deletion with Open Addressing
+ When we get to deletion we have a new problem
```pseudocode
set count to 0
repeat
	set probe to hash(key, count)
	if table(probe) is equal to key then
		delete item # Problem
		return
	else
		increment count
	end if
until table(probe) is empty or count is equal to n
```
+ How, exactly do we delete the item?
+ If we simply replace the item with our empty value we will have an issue:
	+ What if the key we next search for is after the probe corresponding to the deleted key's location.
	+ If, in our previous example, we delete 899, where h(899, 0)=9, and then search for 549, where the sequence of hash values are, 9, 8, 4...
		+ We test D(9) and discover it has the value empty.
		+ We conclude that 549 is not in the table.
		+ Wrong! It is in D(4).
+ To fix this we need a second special value, deleted.
![400](11.Deletion_fixed.png)
+ Our deletion process becomes:
```pseudocode
Set count to 0
repeat
	Set probe to hash(key, count)
	if table(prob) is equal to key then
		table(probe) is equal to deleted
		return
	else
		count++
	end if
until table(probe) is equal to empty or count is equal to n
return not found
```
+ This fixes search but introduces a problem with insertion.
### Open Addressing Hash Functions
+ One question remains.
+ Can we find a function `h(k, i)` which is:
	+ Easy to compute;
	+ Produces a permutation of $\{0,1,\ldots,m-1\}$ as i varies over $\{0,1,\ldots,m-1\}$?
### Strategy 1: Linear Probing
--- start-multi-column

+ In this approach we simply take a standard hash function, $h(k)$ and compute the probe $p(k,i)$ as follows:
	+ $p(k,i)=(h(k)+i)\text{mod m}$
+ In other words, we simply at sequential entries in the dictionary starting at the entry corresponding to $h(k)$.
	+ This is certainly easy to compute.
	+ It does satisfy the permutation.
+ There are $0(m)$ distinct probing sequences.
+ Is it any good?
	+ No! It produces sets of consecutive occupied slots.
	+ Primary Clustering - tendency to create long runs of filled slots near the hash position of keys.
+ The bigger the cluster, the more likely it is to be hit and it gets even bigger!

--- end-column ---

![200](11.LinearProbing.png)

--- end-multi-column
### Strategy 2: Double hashing
+ In this strategy we have two standard hash functions, $h_{1}(k)$ and $h_{2}(k)$.
+ We compute $p(k)$, our probe value as follows:
	+ $p(k,i)=(h_{1}(k)+i\times h_{2(k}))\text{ mod m}$
+ Do we still satisfy our requirements?
	+ This is still easy to compute.
	+ Do we always get a permutation?
		+ No.
		+ Unless we are clever in how we define $h_{2}$.
### Choose $h_{2}$
+ We need $h_{2}(k)$ to be relatively prime to $m$.
	+ i.e. $h_{2}(k)$ and $m$ must have no common factors except 1.
+ This is easy in many cases.
+ If we select $m$ to be a power of 2; say $m=2^{r}$ then all we need is for $h_{2}(k)$ to always be an odd number.
+ For example, if we have a standard hash function $h'(k)$, we can create $h_{2}(k)$ as follows:
	+ $h_{2}(k)=(2h'(k)+1)\text{ mod m}$
+ There are $\theta(m^{2})$ probing sequences
	+ Each possible $(h_{1}(k), h_{2}(k))$ pair yields a distinct probe sequence.
## Table Doubling
+ Once again, we need to expand the dictionary whenever it becomes too full.
+ What does "too full" mean in this case?
	+ We define the occupancy of a table, $\alpha$, to be the ratio of $n$, the number of entries $m$, the number of slots.
		+ $\alpha = \frac{n}{m}$
		+ $0\le\alpha\le1$
	+ We can show that the average cost of an operation on a table with occupancy $\alpha$ is in $\theta(\frac{1}{1-\alpha})$.
	+ In practice we want this value to be  reasonably close to 1 so we double as soon as $\alpha$ exceeds 0.5 or thereabouts.
	+ This keeps operations between $\theta(1)$ and $\theta(2)$.
## Insertion Revisited
+ We not that we can insert a new item into the dictionary in two circumstances
```pseudocode
D(i) == empty
D(i) == deleted
```
+ We modify our insert process as follows:
```pseudocode
Set count to 0
repeat
	Set probe to hash(key, count)
	if table(probe) is equal to empty or table(probe) is equal to deleted then
		store item in table(probe)
		return
	else
		Increment count
	end if
until count is equal to n
return no room
```
+ Now we can insert into the first vacant slot, empty or deleted, that we find in the table.
## Search Revisited
+ Because empty and deleted are different, we do not have to modify our search procedure.
+ The search will skip over deleted records because they do not match the key but will still terminate when it reaches an empty record.
## An Important Note on $\alpha$
+ When calculating the occupancy value, $\alpha$, we must count slots with a value of deleted as containing data.
+ This is because some operations, notably searching, treat deleted records as still containing data.
+ Slots containing deleted may be removed in two ways:
	+ Being overwritten with valid data as a result of an insert operation
	+ Being cleaned up when the table is expanded.
+ If we did not count deleted records in calculated $\alpha$ we could have a notionally empty table in which every slot was deleted.
+ Search (and delete) in this table would be $\theta(m)$, not $\theta(1)$, as we might expect.
## Chaining vs. Open Addressing
+ So, which is the better scheme?
+ Open Addressing:
	+ Uses less memory - no need for pointers
	+ Is faster - provided $\alpha$ is kept below 0.5
	+ Is a little harder to implement and understand.
	+ Is clean - one data structure, the array.
+ Chaining:
	+ Uses more memory.
	+ Is faster - if we are not careful with open addressing.
	+ Is a little easier to implement and understand.
	+ Is a bit messy - arrays of linked lists.