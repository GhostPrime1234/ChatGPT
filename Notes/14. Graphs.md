---
type: topic
academic_year: 2
subject_code: CSCI203
subject_name: Algorithms And Data Structures
topic_number: 14
title: Graphs (1)
cssclass: flatBlue, wideTable
---
< [[__CSCI203 MOC.md|Return to CSCI203 MOC]]

## Graph $G=(V,E)$

| Undirected Graph                                                                                                                                                 | Directed Graph                                                                                                                    |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| $V=\{v_{1},v_{2},v_{3},v_{4}, v_{5}\}$  <br> $E=\{(v_{1}, v_{2}), (v_{1}, v_{5}), (v_{2}, v_{4}), (v_{2}, v_{5} ), (v_{2},v_{3}), (v_{3},v_{4}),(v_{4},v_{5})\}$ | $V=\{v_{1},v_{2}, v_{3}, v_{4}\}$   $E=\{(v_{1},v_{2}),(v_{2},v_{4}),(v_{3},v_{1}),(v_{3},v_{2}), (v_{4},v_{3}), (v_{4},v_{4})\}$ |
| ![](14.UndirectedGraph.png)                                                                                                                                      |                                                                                                                                   |
## Sparse vs Dense Graphs
+ Sparse graphs
	+ Those for which $|E|$ is much less than $|V|^{2}$
+ Dense graphs
	+ Those for which $|E|$ is close to $|V|^{2}$
+ When expressing the running time of an algorithm, it is often in terms of both $|V|$ and $|E|$. In asymptotic notion, we drop the cardinality, e.g. $O(V+E)$
## Abstract Notation
+ IF we ignore the detail of how, exactly, we store the graph and represent it using the following abstract notation.
	+ $G=\{\text{Adj}(v_{1}),\text{Adj}(v_{2}),\ldots,\text{Adj}(v_{|v|})\}$
+ Here, G is defined as the addressable set of elements $Adj(v_{i})$, where $Adj(v_{i})$ is the set of vertices directly reachable from vertex $v_{i}$.
+ This representation allows us to use any appropriate data structure to implement the graph:
	+ Arrays
	+ Hash tables
	+ Matrices
## Representing a graph
+ The best way to represent a graph in a computer depends no how the graph is to be used.
+ The obvious representation - an array of vertices and an array of edges - is probably the worst possible way!
+ Two common representations are: Adjacency List; Adjacency Matrix
## Adjacency Lists
### Directed Graph
## Adjacency Matrix
## Graph Search
+ A common problem involving graphs is Graph Search.
+ This involves '*exploring*' the graph in some systematic way starting at vertex $s$ and visiting the othe reachable vertices by following edges from on vertex to the next.
+ This can be done in more than one way, as we shall see.
+ Graph search has many real life applications, including:
	+ Web crawling
	+ Network broadcast
	+ Social networking
	+ Garbage collection
	+ Solving puzzles and games
## Breadth-First Search (BFS)
+ Our goal is to list all the vertices that are reacable from some starting node $s\in V$ by following edges.
+ To do this in $\theta(|V|+|E|)$ time.
+ Strategy:
	+ List all the nodes reachable from s in 0 moves
	+ List all the new nodes reachable from s in 1 move
	+ List all the new nodes reachable from s in 2 moves
	+ List all the new nodes reachable from s in n moves.
+ Note: A new node is one that has not already been visited.
+ This avoids duplicate nodes in our result.
### BFS Algorithm
```pseudocode
procedure BFS(s, Adj):
	v: set of verticies
	q: queue of vertices
	c[vertex]: visited or not
	v={}
	q.enqueue(s); c[s] = visited
	while q is not empty do
		Set current to q.dequeue()
		add current to v
		for each n in adj(current) do
			if n is not in v and not visited then
				q.enqueue(n)
				Set c[n] to visited
			end if
		end for
	end while
	return v
end BFS
```
### BFS Example
+ $BFS(v_{1},\text{Adj})$![300](14.BFS_Example.png)
+ Current node: c
+ Visited nodes:   $v\quad\quad v_{1}\quad v_{2}\quad v_{4}\quad v_{3}\quad v_{5}\quad v_{7}\quad v_{6}$
+ Pending nodes: $q\quad\quad v_{1}\quad v_{2}\quad v_{4}\quad v_{3}\quad v_{5}\quad v_{7}\quad v_{6}$

| c     | v                                 | q             |
| ----- | --------------------------------- | ------------- |
|       | $\{\}$                            | $\{v_{1}\}$   |
| $v_1$ | $\{v_1\}$                         | $\{v_2,v_4\}$ |
| $v_2$ | $\{v_1,v_2\}$                     | $\{v_4,v_3\}$ |
| $v_4$ | $\{v_1,v_2,v_3\}$                 | $\{v_4,v_3\}$ |
| $v_3$ | $\{v_1,v_2,v_4,v_3\}$             | $\{v_5,v_7\}$ |
| $v_5$ | $\{v_1,v_2,v_4,v_3,v_5\}$         | $\{v_5,v_7\}$ |
| $v_7$ | $\{v_1,v_2,v_4,v_3,v_5,v_7\}$     | $\{v_6\}$     |
| $v_6$ | $\{v_1,v_2,v_4,v_3,v_5,v_7,v_6\}$ | $\{\}$        |
### Keeping Track
+ Sometimes we wish to keep track of how we got to each node in a graph.
+ To achieve this we need only make a small change to the BFS algorithm.
	+ We add an array, or hash table, $P$, indexed by each vertex we reached, containing the parent of each vertex, the vertex from which we reached it.
+ Clearly:
	+ The parent of our starting vertex s is empty; $p(s)=null$
	+ The parent of each vertext directly connect to s is s
+ The modified BFS is shown on the next slide.
### BFS with Parents
```pseudocode
procedure BFS_Parent(s, adj):
	v: set of vertext
	q: queue of vertex
	c[vertex]: visited or not
	p[vertex]: array of vertex
	Set v to {}
	q.enqueue(s); 
	Set c[s] to visited
	Set p[s] to null
	while q is not empty do
		Set current to q.dequeue()
		add current to v
		for each n in adj(current) do
			if n is not in v and not visited then
				q.enqueue(n)
				Set c[n] to visited
				Set p[n] = current
			 end if
		 end for
	 end while
 end BFS_Parent
```
#### BFS with Parent Tracking, an Example
$BFS(v_{1},Adj)$

| c     | v                                 | q                 | p                                            |
| ----- | --------------------------------- | ----------------- | -------------------------------------------- |
|       | $\{\}$                            | $\{v_{1}\}$       | $\{\emptyset,?,?,?,?,?,?\}$                  |
| $v_1$ | $\{v_1\}$                         | $\{v_2,v_4\}$     | $\{\emptyset,v_{1},?,ùë£_1,?,?,?\}$           |
| $v_2$ | $\{v_1,v_2\}$                     | $\{v_4,v_3\}$     | $\{\emptyset,v_{1},v_{2},ùë£_1,?,?,?\}$       |
| $v_4$ | $\{v_1,v_2,v_3\}$                 | $\{v_3,v_5,v_7\}$ | $\{\emptyset,v_{1},v_{2},ùë£_1,?,?,?\}$       |
| $v_3$ | $\{v_1,v_2,v_4,v_3\}$             | $\{v_5,v_7\}$     | $\{\emptyset,v_{1},v_{2},ùë£_1,v_4,?,v_4\}$   |
| $v_5$ | $\{v_1,v_2,v_4,v_3,v_5\}$         | $\{v_7,v_6\}$     | $\{\emptyset,v_{1},v_{2},ùë£_1,v_4,?,v_4\}$   |
| $v_7$ | $\{v_1,v_2,v_4,v_3,v_5,v_7\}$     | $\{v_6\}$         | $\{\emptyset,v_{1},v_{2},ùë£_1,v_4,v_5,v_4\}$ |
| $v_6$ | $\{v_1,v_2,v_4,v_3,v_5,v_7,v_6\}$ | $\{\}$            |                                              |
### Using the Parents
+ To find the path from vertex s to any given vertex, d, we simply list the sequence
	+ `d;`
	+ `p[d]`
	+ `p[p[d]]`
		...
	+ `p[p[...p[[d]]...]`\
	+ Until we reach s
+ The reverse of this list is the path we seek
### Efficiency and Property of BFS
+ We note that, in the worst case, we visit all vertices in the graph.
+ For each vertex in the graph we traverse each of its edges.
+ The complexity of BFS is, therefore, $O(|V|+|E|)$
	+ Strictly speaking, for undirected graphs, $O(|V|+2x|E|)$
	+ For directed graphs, $O(|V|+|E|)$
+ BFS will always find the shortest path from s to x, assuming a path exists, while DFS will not.
	+ Shortest is the fewest vertices in the path.
## Depth First Search (DFS)
+ The goal of DFS is the same as BFS:
	+ List all vertices reachable from the starting vertex s.
+ Strategy:
	+ For each vertex v in Adj(s):
		+ Perform DFS on v.
	+ This is a recursive implementation of DFS.
+ We can also perform DFS using a non-recursive algorithm.
+ All we need to do is replace the queue from BFS with a stack.
### DFS Algorithm
```pseudocode
procedure DFS(s,adj):
	v: set of vertext
	k: stack of vertex
	c[vertex]: visited or not and is initialised as false
	p[vertex]: array of vertex
	Set v to {}
	k.push(s)
	Set c[s] to visited
	Set p[s] to null
	while k is not empty do
		Set current to k.pop()
		if c[v] = false then
			add current to v
			set c[v] to visited
		else
			continue // stop and begin a new while loop
		end if
		for each n in adj(current) do
			if n is not in V then // c[n] = false
				k.push(n)
				set p[n] to current
			end if
		end for
	end while
	return v
end DFS
```
### DFS Example
![](14.DFS_Graph.png)

| c     | v                                       | q                     | p                                          |
| ----- | --------------------------------------- | --------------------- | ------------------------------------------ |
|       | $\{\}$                                  | $\{v_{1}\}$           | $\{\emptyset,?,?,?,?,?,?\}$                |
| $v_1$ | $\{v_1\}$                               | $\{v_2,v_4\}$         | $\{\emptyset,v_{1},?,ùë£_1,?,?,?\}$         |
| $v_4$ | $\{v_1,v_4\}$                           | $\{v_2,v_5,v_7\}$     | $\{\emptyset,v_{1},?,ùë£_1,v_4,?,v_4\}$     |
| $v_7$ | $\{v_1,v_4,v_7\}$                       | $\{v_2,v_5,v_5,v_6\}$ | $\{\emptyset,v_{1},?,ùë£_1,v_7,v_7,v_4\}$   |
| $v_6$ | $\{v_1,v_4,v_7,v_5\}$                   | $\{v_2,v_5,v_5,v_5\}$ | $\{\emptyset,v_{1},?,ùë£_1,v_6,v_7,v_4\}$   |
| $v_5$ | $\{v_1,v_4,v_7,v_6,v_5\}$               | $\{v_2,v_5,v_5\}$     | $\{\emptyset,v_{1},?,ùë£_1,v_6,v_7,v_4\}$   |
| $v_5$ | $\{v_1,v_4,v_7,v_6,v_5\}$               | $\{v_2,v_5\}$         | $\{\emptyset,v_{1},?,ùë£_1,v_6,v_7,v_4\}$   |
| $v_5$ | $\{v_1,v_2,v_4,v_3,v_5,v_7\}$           | $\{2\}$               | $\{\emptyset,v_{1},?,ùë£_1,v_6,v_7,v_4\}$   |
| $v_2$ | $\{v_1,v_2,v_4,v_3,v_5,v_7,v_{2}\}$     | $\{v_3\}$             | $\{\emptyset,v_{1},?,ùë£_1,v_6,v_7,v_4\}$   |
| $v_6$ | $\{v_1,v_2,v_4,v_3,v_5,v_7,v_{2},v_6\}$ | $\{\}$                | $\{\emptyset,v_{1},v_2,ùë£_1,v_6,v_7,v_4\}$ |
## Some Notes
+ BFS and DFS will visit the same set of vertices given the same starting vertex.
	+ All that changes is the order they are visited.
+ The edges traversed in performing a DFS or BFS form a tree.
	+ If the graph is connected this is a spanning tree.
+ BFS and DFS work on directed as well as on undirected graphs.
## Traverse a Maze
+ If we modify DFS to track parents we can use it as a means of finding our way through a maze.
+ Consider the following maze:
	![200](14.DFS_mazeTraversal.png)
	+ We can represent it as a graph with 18 vertices, $a,b,\ldots p,s\text{ and }x$
	+ An edge exists in this graph if two vertices are adjacent and do not have a wall between them.
	+ We start the maze at vertex $s$ and exit to vertex $x$
+ If we try directions in the sequence:
	+ Down; Up;
	+ Left; Right
	+ (We need to stack in the reverse order; R, L, U, D)
## DFS of $G(V,E)$ - $DFS\_ALL(G)$
+ If the graph is not connected, or is directed and not strongly connected, we may not reach every vertex with a single call to DFS.
+ In this case we will need to call DFS repeatedly, once for each vertex we have not yet visited.
+ We can do this easily with the following procedure, $DFS\_ALL(G)$:
	+ <span style=color:red>DFS of a graph can reveal important information of the structure of the graph.</span>
### DFS_ALL(G)
+ **Input:** $G=(V,E)$, directed or undirected. No source vertex given!
+ **Output:** 2 timestamps on each vertex:
	+ `d[v]` = discovery/exploring time
	+ `f[v]` = finishing time
	+ These will be useful for other algorithms
+ Will methodically explore every edge.
	+ Start over from different vertices as necessary.
+ As soon as we discover a vertex, explore from it.
	+ Unlike BFS, which puts a vertex on a queue so that we explore from it later.
+ As DFS progresses, every vertex has a colour:
	+ WHITE = undiscovered
	+ GRAY = discovered, but not finished (not done exploring from it)
	+ BLACK = finished (have found everything reachable from it)
+ Discovery and finish times:
	+ Unique integers from 1 to $2|V|$
	+ For all $v,d[v]\le f[v]$
	+ In other words, $1\le d[v]\lt f[v]\le 2|V|$
+ Uses a global timestampt *time*
```pseudocode
procedure DFS_ALL(G)
	for each u in V 
		do color[u] <- WHITE
	time <- 0
	for each u in V
		do if color[u] = WHITE then DFS-VISIT(u)
	DFS-VISIT(G,u)
	color[u] <- GRAY // discover u
	time <- time + 1
	d[u] <- time
	for each v in Adj[u] // explore (u, v)
		do if color[v] = WHITE then DFS-VISIT(v)
	color[u] <- BLACK
	time <- time + 1
	f[u] <- time
end DFS_ALL
```
## Edge Classification
+ If we perform a DFS on a graph we can classify the edges of a graph:
	+ `Tree edges`: these form part of the search tree (or forest);
	+ `Forward edges`: these lead from a vertex to a descendent
	+ `Backward edges`: these lead from a vertex to an ancestor
	+ `Cross edges`: these are all the edges that are left - they connect unrelated vertices 
+ Vertex v is descendant of vertex u in depth first search if and only if v is discovered during the time which u is grey.
### Example
+ In the Graph shown:
![200](14.EdgeClassification_Example.png)
+ The edges are classified as follows:
	+ Tree edges (in red)
	+ Forward edges (in green)
	+ Backward edges (in orange)
	+ Cross edges (in cyan)
## How to Tell?
+ `DFS_ALL(G)` has enough information to classify some edges as it encounters them.
+ The key idea is that when we first explore an edge $(u,v)$, the colour of vertex $v$ tells us something about the edge $(u,v)$:
	+ WHITE indicates a <span style=color:red>tree edge</span> (in the search tree)
	+ GRAY indicates a <span style=color:red>back edge</span>
	+ BLACK indicates a <span style=color:red>forward or cross edge</span>
		+ $d[u]\lt f[v]$ indicates forward edge
		+ $d[u]\gt f[v]$ indicates cross edge.
## Cycle Detection
+ We define a cycle as any sequence of edges that form a closed loop in a graph
![200](14.Graph_Cycle.png)
+ The edges $(b,e),(e,d)$ and $d,b$ form a cycle
+ Finding these cycles is trivial once we have classified all the edges:
	+ G has a cycle, if and only if, `DFS(G, Adj)` contains at least one backward edge.
### Finding Cycles
+ Once we find a backward edge, $(v_{0},v_{k})$, finding the cycle if forms part of is easy:
	+ Simply follow the parent sequence, back from $v_{0}$ until we reach $v_{k}$.
	+ These edges plus the backward edge must form the cycle.
+ Our next problem, Topological Sort, requires that the graph to be <span style=color:red>acyclic</span>:
	+ It has no cycles
+ We will look at topological sort by way of a simple application, job scheduling.
## Topological Sort - Job Searching
+ Consider a directed, acyclic graph (DAG) in which:
	+ The vertices each represent a task to be performed;
	+ The edges represent an order in which the tasks may be performed.
		+  If edge $(v_{i},v_{j})$ exists in the graph, task $v_{i}$ must be completed before we start task $v_{j}$.
+ Our problem is to find a feasible sequence of tasks:
	+ An order in which the tasks may be performed which does not conflict with the order required by all of the edges.
+ You can only do one task at a time.
+ Consider the task of getting dressed
### Algorithm
+ TOPOLOGICAL-SORT(G)
	+ call DFS_ALL(G) to compute finishing times $f[v]$ for each vertex $v$/
	+ As each vertex is finished, insert it onto the front of a linked list.
	+ Return the linked list of vertices.
+ We can perform a topological sort in time $\theta(V+E)$ since depth-first search takes $\theta(V+E)$ and it takes $O(1)$ time to insert each of the $|V|$ vertices onto the front of the linked list.
## Getting Dressed 
### Vertices
+ We can label our tasks (vertices), in alphabetical order:
	+ Put on undershorts
	+ Put on pants
	+ Put on belt
	+ Put on shirt
	+ Put on tie
	+ Put on jacket
	+ Put on socks
	+ Put on shoes
	+ Put on watch
### Edges
+ We can then determine the edges as $(a,b)$; perform a before b:
--- start-multi-column

+ (undershorts, shoes)
+ (undershorts, pants)
+ (pants, belt)
+ (pants, shoes)
+ (shirt, belt)
+ (shirt, tie)
+ (tie, jacket)
+ (socks, shoes)
+ (belt, jacket)

--- end-column ---

![300](14.GettingDressed_Graph.png)

--- end-multi-column
+ Not vertex "watch" do not appear in the edge list:
	+ We can do this task at any time.
+ Taken together, the vertices and edges form a DAG.
### The Graph
+ The resulting graph is shown to the right: 
+ We perform our topological sort as follows:
	+ Conduct a DFS of the graph
	+ List the vertices in reverse order of finish time
![300](14.GettingDressed_Graph.png)
+ In practice we construct a list of the vertices as each is popped from the stack and then print the list backwards.
![500](14.GettingDress_StackPop.png)
<span style=color:red;font-weight:bold>The same graph shown topologically sorted, with its vertices arranged from left to right in order of decreasing finishing time. All directed edges go from left to right.</span>
## One of Many
+ The example we just saw gives us one of the many possible topological sorts for the "*getting dressed*" graph.
+ If we visited the vertices in a different order we would probably get a different sort order.

## Weighted Graphs
+ Frequently, we find that travelling along an edge in a graph has some associated cost (or profit) associated with it:
	+ The distance along the edge; The cost of petrol; The time of travel; etc.
+ We call these Weighted Graphs
+ We call the edge values weights.
### Definition
+ We extend our previous graph definition as follows:
	+ A weighted graph, $G$, consists of the ordered sequence $(V,E,W)$ where $V$ and $E$ are the vertices and edges and $W$ are the edge weights.
--- start-multi-column

+ Consider the weighted graph shown to the right:
	+ $V=\{a,b,c,d,e\}$
	+ $E=\{(a,b),(a,c),(a,d),(a,e),(c,b),(d,b),(d,c),(e,d)\}$
+ W is a function that maps edges to weights:
	+ e.g., $W((a,b))=50$

--- end-column ---
![400](16.WeightedGraph_GraphExample.png)

--- end-multi-column
### Representation
+ We can extend our adjacency matrix representation of a graph by replacing the zero-one existence value with the edge weight.
![400](16.WeightedGraph_Representation.png)
+ In this example, "-" indicates that no edge exists.
+ The actual value will depend on the nature of the weights:
	+ E.g. if all weights are non-zero use 0.
	+ We often use $\infty$ to represent missing edges.
	+ For some applications, it is more convenient to put 0's on the main diagonal of the adjacency matrix.
+ We can also use the adjacence list representatioN:
	+ We just need to pair each edge with its corresponding weight.
![400](16.WeightedGraph_AdjacenyListRepresentation.png)
### Shortest Path
+ A common problem associated with weighted graphs is finding the shortest path between vertices.
+ There are several versions of this problem:
	+ Single Source-All destinations
	+ Single Source - Single Destination
	+ All Sources - All Destinations
	+ All Sources - Single Destination
+ Each has applications in the real world.
#### Single Source - All Destinations
+ The problem is stated as follows:
	+ Starting at some vertex, $s$, find the shortest path from $s$ to each other reachable vertex in the graph.
+ As we shall se later, the solution to this problem can be used as a basis for solving all of the other shortest path formulations.
+ We will examine two algorithms for solving this problem:
	+ Dijkstra;
	+ Bellman Ford.
+ Each has advantages in certain cases.
### Negative Weights
--- start-multi-column
+ There is not a prior reason why the edge weights in a graph must be positive, but negative weights can cause problems.
+ Consider the following graph:
	+ Clearly the length of the shortest path from a to d is 3.
+ But what if we changes the weights?
	+ Now what is the shortest path?
+ The problem is that we now have a negative cost cycle\.

--- end-column ---

![](16.WeightedGraph_NegativeWeights.png)
--- end-multi-column
## What is a Path
+ While it is obvious what a path is, we should define it formally.
+ A path $p$ from vertex $v_{1}$ to vertex $v_{k}$ is an ordered sequence $(v_{1},v_{2},\ldots,v_{k-1},v_{k})$ where each edge $(v_{i},v_{i+1})\in E$
+ The weight of path $p,W(p)$ is the sum of the edge weights:
	+ $W(p)=\sum\limits^{k-1}_{i=1}W(v_{i},v_{i+1})$
## What doesn't work?
+ We might be tempted to try using a technique we already know for traversing a graph, breadth first search, in our search for shortest paths.
+ Unfortunately, this does not always work.
+ The two definitions of shortest path:
	+ Fewest edges
	+ Smallest weight
+ May not always coincide.
## Dijkstra's Algorithm
+ This algorithm works by dividing the vertices into two sets, $S$ and $C$.
+ At each iteration, $S$ contains the set of nodes that have already been chosen.
	+ This is the selected set.
+ At each iteration, $C$ contains the set of nodes that have not yet been chosen:
	+ This is the candidate set.
+ At each step we move the node which is cheapest to reach from $C$ to $S$
+ We also need a function $D$ such that $C(c_{i})$ is the shortest distance we have so far found from vertex $s$ to vertex $c_{i}$ in the candidate set $C$.
+ Initially:
	+ The selected set, $S$ just contains the start vertex;
	+ The candidate set, $C$, contains all the other vertices
	+ The distance function, $D()$ has value 0 for vertex s and is infinite for all other vertices.
+ We start by re-evaluating $D$ for each vertex directly reachable from vertex $s$
### Example
![400](16.Dijkstra'sAlgorithm_Example.png)
+ Step 0:
	+ $S=\{a\}$
	+ $C=\{b,c,d,e\}$
	+ $D=\{50,30,100,10\}$
	+ We now select the minimum of D, $D(e)=10$
+ Step 1: move vertices e from C to S.
	+ $S=\{a,e\}$
	+ $C=\{b,c,d\}$
	+ We now update $D$ by looking at vertices we can reach from vertex e
	+ $D=(50,30,100\to20)$
	+ We now select the minimum value of D, $D(d)=20$
+ Step 2: move vertex d from C to S.
	+ $S=\{a,e,d\}$
	+ $C=\{b,c\}$
	+ We can now update D, by looking at vertices we can reach from vertex $d$
	+ $D=(50\to40, 30)$
	+ We now select the minimum value of $D$, $D(c)=30$
+ Step 3: move vertex c from $C$ to $S$.
	+ $S=\{a,e,d,c\}$
	+ $C=\{b\}$
	+ We can now update $D$ by looking at vertices we can reach from vertex c.
	+ $D=(40\to35)$
	+ We now select the minimum valeu of $D,D(b)=35$
+ Step 4: move vertex $b$ from $C$ to $S$.
	+ $S=\{a,e,d,c,b\}$
	+ $C=\emptyset$
	+ We now have no remaining candidate verticies; we have finished.
	+ $W(a)=0,W(e)=10,W(d)=20,W(c)=30,W(b)=3r5$
### Pseudocode
```pseudocode
Procedure Dijkstra(G: array[1..n, 1..n]): array[2..n]
	D: array[2..n]
	C: set = {2, 3, ..., n}
	for i = 2 to n do
		set D[i] to G[1,i]
	end for
	repeat
		set v to the index of the minimum D[v] not yet selected
		reomve v from C # Implicitly add v to S
		for each u ‚àà C do
			set D[u] to min(D[u], D[v] + G[v, u])
		end for
	until C contains no reachable ndoes
	return D
end Dijkstra
```
## Recording Paths
+ Like the basic DFS, Dijkstra's algorithm does not record the shortest path to each vertex just its total weight.
+ Also, like DFS, we can use a parent record, $p$, to keep track of how we reach each vertex.
+ This entails a couple of minor changes to the algorithm
### Dijkstra's Algorithm: Path Recording
```pseudocode
Procedure Dijkstra(G: array[1..n, 1..n]): array[2..n]
	D: array[2..n], P: array[2..n]
	C: set = {2, 3, ..., n}, S: set = {}
	for i = 2 to n do
		set D[i] to G[1,i]
		set P[i] to 1
	end for
	repeat
		set v to the index of the minimum D[v] not yet selected
		move v from C to S
		for each u ‚àà C do
			if D[u] > (D[v] + G[v, u]) then
				set D[u] to min(D[u], D[v] + G[v, u])
				set p[u] to v
			end if
		end for
	until C contains no reachable ndoes
	return D
end Dijkstra
```
## Analysis
+ The complexity of Dijkstra's Algorithm is $\theta(V\times\log V+e)$
	+ How?
+ Usually, $E\gt V$, in fact, in the worst case - $E(\in\theta(V^{2}))$ - for a complete graph.
+ There is one disadvantage: The algorithm only works if all edge weights are non-negative.
+ If we have negative edge weights, and especially negative edge cycles, we need a different algorithm.
## The Bellman-Ford Algorithm
+ Independently invented by both Bellman and Ford.
+ Works with graphs that have negative edge weights.
+ Identifies negative cycles and vertices with a negative cycle on their path.
+ Finds correct path and path length for all other vertices.
### A Graph with a negative cycle
![400](16.Bellman-Ford_GraphNegativeCycle_example.png)
+ Consider the graph shown above:
+ Because the edges between vertices 4, 7 and 6 form a cycle whose total weight is -3, we can reduce the cost of any vertex with a path through any of these vertices as much as we like.
+ Not that some vertices, namely 2 and 3, will have well defined minimum costs of 2 and -4 respectively.
### The Algorithm
+ **Input:** Graph $G=(V, E, W)$ and a source $s$
+ **Output:** Shortest distance to all vertices from $s$. If there is a negative weight cycle, then shortest distances are not calculated, negative weight cycle is reported.
1. Initialisation - $D[v]=\infty,v\in V,\text{except }V[s]=0$
2. Calculates shortest distances. Do following $|V|-1$ times
	* Do following for edge u-v in E
		* If $D[v]\gt D[u]+W((u,v))$, then update $D[v]$
		     $D[v]=D[u] + W((u,v))$
 3. Reports if there is a negative weight cycle in graph. Do following for each edge u-v in E
	 * If $D[v]\gt D[u]9W((w,v))$, then "*Graph contains negative weight cycle*"
### Different from Dijkstra's
+ Unlike Dijkstra's algorithm, in which update only the most promising (next lowest cost) vertex at each iteration, Bellman-Ford updates every vertex at each iteration.
+ This means that each iteration of Bellman-Ford involves more work than the corresponding iteration of Dijkstra.
### Analysis
+ Bellman-Ford performs this major loop $|V-1|$ times.
+ Inside this loop it checks every edge; $|E|$ operations.
+ Finally, it does another $|E|$ checks for potential cycles.
+ Overall, Bellman-Ford has $\theta(|V|\times|E|)$ complexity
### Notes on Bellman-Ford Algorithm
+ Bellman-Ford algorithm can handle directed and undirected graphs with non-negative weights.
+ However, it can only handle directed graphs with negative weights, as long as we don't have negative cycles.
+ When the graph has a negative cycle, Bellman-Ford algorithm can detect the cycle, but it won't be able to find the shortest paths in this case.
## Bellman-ford vs Dijkstra
--- start-multi-column

```pseudocode
function bellmanFord(G,s)
	for each vertex V in G do
		set distance[V] to infinite
		set previous[v] to None
	end for
	
	set distance[S] to 0

	for each vertex V in G do
		for each edge (U, V) in G do
			set tempDistance to distance[U] + edge_weight(U, V)
			if tempDistance < distance[V] then
				set distance[V] to tempDistance
				set previous[v] to U
			end if
		end for
	end for

	for each edge (U, V) in G do
		if distance[U] + edge_weight(U, V) < distance [V] then
			output Error: Negative Cycle Exists
		end if
	end for

	return distance[], previous[]
end bellmanFord
```

--- end-column ---

```pseudocode
function dijkstra(G, s) 
	for each vertex V in G
		set distance[v] to infinite
		set previous[V] to null
		if V != S then add V to Priority Que
	end for

	set distance[S] to 0

	while Q is not empty do
		set U to Extract minimum from Q
		for each unvisited neighbour V of U do
			set tempDistance to distance[U] + edge_weight(U, V)
			if tempDistance < distance[V] then
				set distance[V] to tempDistance
				set previous[V] to U
			end if
		end for
	end while

	return distance[], previous[]
end dijkstra
```
--- end-multi-column
## Tweaking Dijkstra
+ As we have already seen, Dijkstra's  algorithm provides an effective solution strategy for solving the <span style=color:red>single source-all destinations</span> version of the shortest path problem.
+ We will now look at a few modifications of Dijkstra that will:
	+ Improve its practical performance and/or extend its range of applicability.
### Dijkstra's Algorithm
```pseudocode
procedure Dijkstra(G: array[1..n, 1..n]): array[2..n]
	D: array[2..n], P: array[2..n]
	C: set = {2, 3, ..., n}
	for i = 2 to end do
		set D[i] to G[1, i]
		set P[i] to 1
	end for
	repeat
		set v to the index of the minimum D[v] not yet selected
		remove v from C // and implicitly add v to S
		for each u ‚àà C do
			set D[u] to min(D[u], D[v] + G[v,u])
			set p[u] to v
		end for			
	until C contains no reachable nodes
	return D
end Dijkstra
```
### Overall Efficiency
+ The algorithm as you have seen it so far has efficiency $O(|V^{2}|+|E|)$.
+ But it was $O(|V|\times\log|V|+|E|)$, How come?
+ The answer is simple:
	+ As presented, we find the next vertex to select by searching a list of candidate vertices and selecting the vertex with minimum $D$ value.
		+ This is a linear search process; $O(|V|)$
		+ We do this for each vertex, also $O(|V|)$
		+ This is $O(|V|^{2})$
### Reaching Peak Efficiency
+ Replace the candidate list/array $C$ with a priority queue (or a heap), ordered on $D(v)$.
+ Now:
	+ Finding the best candidate is $O(1)$
	+ Update $C$ is $O(log|V|)$.
+ Now, over all vertices, we have $|V|\times\log|V|$
### All Sources - Single Destination
+ In this case, rather than finding paths from a starting vertex , $s$, to all other reachable vertices we are looking for the shortest paths to some goal vertex, $g$, from all possible starting vertices.
+ How do we do this? Run Dijkstra backwards.
+ Specifically:
	+ Redefine $Adj(v)$ to be the list of set of vertices leading to vertex $v$ instead of reachable from v.
	+ Start with $D(g)=0$ instead of $D(s)=0$
	+ Let $P(v)$ indicate the next vertex in the path instead of the prior index.
	+ Let the selected set, $S$, start at $\{g\}$ instead of $\{s\}$
### Single Source - Single Destination
+ What do we do if, rather than looking for the shortest paths from a start vertex, $s$, to all other vertices, we wish to find the shortest path from s to a specific goal vertex, $g$?
+ The answer is easy: Stop when vertex $g$ becomes a member of $S$, the selected set.
+ This means that we don't waste time with any vertex further away from $s$ than $g$.
+ This usually reduces the total running time of the algorithm.
+ ==Why?==
	- **Early Termination**: Stop the algorithm as soon as the goal vertex \( g \) is reached, avoiding unnecessary calculations for other vertices.
	- **Reduced Search Space**: Limit the exploration to paths leading directly to \( g \), reducing the number of nodes processed.
	- **Fewer Priority Queue Updates**: Minimize updates to data structures by not considering vertices further away from \( g \).
	- **Improved Efficiency**: Enhance overall performance by saving time and resources on irrelevant computations.
### The Problem with Dijkstra's
+ There is a big problem with using Dijkstra on the single source/single destination problem:
	+ The order in which the vertices are added.
+ Consider the following graph:
	+ Say we want to get from Adelaide to Perth.
+ Dijkstra will add all of the closer vertices first:
	+ Before we ever get close to the path we seek.
![](16.Dikstra_Problem_Graph.png)
### Another Example
**Dijkstra Algorithm**
+ Repeatedly examines the closes not-examined vertex
+ Expands outwards until it reaches the goal.
+ Guarantee to find a shortest path but have a large teal area.
![400](16.Dijkstra_Algorithm_Example2.png)
+ The shortest path is not a straight line when it has obstacles.
+ Works harder but is guaranteed to find a shortest path.
![400](16.Dijkstra_Algorithm_Example2-2.png)
### Fixing the problem
+ So how do we remedy this?
+ Before we get to the answer let us take a step back.
+ Let us <span style=color:red>generalise</span> Dijkstra's algorithm.
+ The key step in the algorithm is on the process by which we select the next vertex.
	+ Specifically, we select the vertex in the candidate set, $C$ for which the overall distance from the source vertex, $s$, is minimised.
	+ $D(v)=P(s,v)$
+ Note that this doesn't involve the goal vertex, $g$.
### Eyes on the Goal
+ What if we could <span style=color:red>bias</span> towards the goal in some way.
+ We can! Essentially, we select the minimum not simply of $D(s,v)$ but, instead of $D(s,v)+H(v,g)$
+ This new function, $H$, is a heuristic; an estimate of the remaining distance from each candidate vertex, $v$, to the goal vertex, $g$
+ What is a good estimator?
## Greedy Best-First Search Algorithm
+ Similar as the Dijkstra algorithm but with a heuristic
+ Instead of selecting the vertex closest to the starting point, it selects the vertex closest to the goal.
+ Not guaranteed to find a shortest path but it is much quicker.
![400](16.GreedyBestFirst_SearchAlgorithm.png)
+ Does less work and faster
+ But the path is not the shortest
![400](16.GreedyBestFirst_SearchAlgorithm2.png)
+ Can we combine the Dijkstra and the Greedy algorithms?
```pseudocode
procedure GBS(start,target) is:
	mark start as visited
	add start to queue
	while queue is not empty do:
		set current_node to vertex of queue with min distance to target
		remove current_node from queue
		for each neightbour n of current_nod do
			if n not in visited then:
				if n is target:
					return n
				else:
					mark n as visited
					add n to queu
				end if
			end if
		end for
	end while
	return failure
```
### A Good Heuristic
+ We require $H(v,g)$ to have one key property:
	+ $H(v,g)\le P(v,g)$
	+ The heuristic estimate must never exceed the actual shortest path length.
	+ This requirement guarantees that the final path we find is still the correct answer.
+ In our example we have a ready-made heuristic the Euclidean (straight-line) distance between $v$ and $g$.
+ Provided the graph behaves according to the rules of geometry there can never be a shorter path than this.
## A*
+ The heuristic modification to the vertex selection rule changes Dijkstra's algorithm into an example of what is called the A* algorithm.
+ Although, in the worst case, $A*$ is no faster than Dijkstra in practice it will generally represent a substantial improvement.
+ Note: the <span style=color:red>trick to A* is finding a good heuristic.</span>
+ The nearer that $H(v,g)$, the estimated minimum path length from $v$ to $g$, is to $P(v,g)$, the actual minimum path length, the faster $A*$ will find the solution.
+ $A*$ algorithm is like Dijkstra's algorithm to find the shortest path.
+ $A*$ algorithm also is like Greedy Best-First-Search in that it can use a heuristic to guide itself.
+ In short, $A*$ algorithm is as reliable as Dijkstra's algorithm and as fast as Greedy Best-First-Search Algorithm.
![](16.Astar_graph.png)
+ $A*$ algorithm finds a path as good as what Dijkstra's algorithm found.
+ But $A*$ algorithm is faster.
+ Because it combines the Dijkstra's algorithm with the heuristics.
![](16.Astart_Graph2.png)
### A* Search Algorithm
1. Initialise the open list - a list of nodes to be explored
2. INitialise teh closed list - a list of nodes in the path so far put the starting node on the opening list (you can leave its f at zero)
3. While the open list is not empty
	1. Find the node with the least f on the open list, call it "q"
	2. pop q off the open list
	3. generate q's non-blocked successors from the 8 ones and set their parents to q
	4. for each success
		1. if successor is the goal stop search
		   successor.g = q.g + distnace between successor and q
		   successor.h = distance from goal to successor
		   // This can be done using many ways, we will discus three heuristics - Manhattan, Diagonal and Euclidean Heuristics
		   successor.f = successor.g + successor.h
		   2. If a node with the same position as successor is in the OPEN list which has a lower f than successor, skip this successor
		   3. If a node with the same position as successor is in the CLOSED list which has a lower f than successor, skip this successor otherwise, add the node to the open list.
		end (for loop)
		5. push q on the closed list
end (while loop)
### Through the Looking Glass
+ Because of the order in which we saw them, it is easy to think of $A*$ as a generalisation of Dijkstra's algorithm.
+ This is not the only, or perhaps even the best, way to view this.
+ Consider instead this viewpoint:
	+ Dijkstra's algorithm is simply a special case of $A*$:
		+ The one with the worst possible choice of $H$.
		+ Specifically, $H(v,g)=0$
### Example
+ We can consider a 2D grid having several obstacles and we start from a source cell (green), $s$, to reach towards a goal cell (red), $z$
+ We want to reach the target cell (if possible) from the starting cell as quickly as possible
![400](16.Astar_Example.png)
In the $A*$ Algorithm
+ At each step, it picks the node (while cell), $x$ according to a value $f(s,z)=g(s,x)+h(x,z)$
	+ $g(s,x)$ - cost/distance to moved from $s$ to $x$
	+ $h(x,z)$ - estimated cost/distance, heuristic, to move from $x$ to $z$.
+ We don't know the actual $f(s,z)$, until we find the path
+ There are many ways to calculate $h()$
## Heuristic $h$
+ We can calculate $g$ but how to calculate $h$?
+ We can do things.
	+ Either calculate the exact value of $h$ (which is certainly time consuming)
	+ Approximate the value of $h$ using some heuristics (less time consuming)
### Exact Heuristic
+ We can find exact values of $h$, but that is generally very time consuming.
+ e.g. Pre-compute the distance between each pair of cells before running the A* Search Algorithm
### Approximation
+ Manhattan Distance - $h=abs(current_cell.x-goal.x)+abs(current_cell.y-goal.y)$
	+ The sum of absolute values of differences in the goal's x and y coordinates and the current cell's x and y coordinates respectively.
	+ When to use this heuristic? - When we are allowed to move only in four directions only (right, left, top, bottom)
![400](16.Heuristic-Approximation_ManhatanDistance.png)
+ Euclidean Distance - The distance between the current cell and the goal cell using the distance formula $h=\sqrt{(current_cell.x-goal.x)^{2}+(current_cell.y-goal.y)^{2}}$
+ When to use this heuristic? - When we are allowed to move in any directions.
![400](16.Heuristic_EuclideanDistance.png)
+ A* Search algorithm would follow path as shown if Euclidean Distance is chosen as a heuristics.
![](Pasted%20image%2020240920113715.png)